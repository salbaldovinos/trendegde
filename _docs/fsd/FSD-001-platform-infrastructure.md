# FSD-001: Platform Infrastructure & DevOps

**TrendEdge -- AI-Powered Futures Trading Platform**

| Field | Value |
|---|---|
| FSD ID | FSD-001 |
| Source PRD | PRD-001 |
| Title | Platform Infrastructure & DevOps |
| Version | 1.0 |
| Status | Draft |
| Author | Generated by Claude |
| Created | 2026-02-11 |
| Classification | CONFIDENTIAL |

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [System Context](#2-system-context)
3. [Functional Specifications](#3-functional-specifications)
4. [Data Specifications](#4-data-specifications)
5. [API Specifications](#5-api-specifications)
6. [UI/UX Specifications](#6-uiux-specifications)
7. [Integration Specifications](#7-integration-specifications)
8. [Security Specifications](#8-security-specifications)
9. [Performance Specifications](#9-performance-specifications)
10. [Testing Specifications](#10-testing-specifications)
11. [Migration & Deployment](#11-migration--deployment)
12. [Open Questions & Assumptions](#12-open-questions--assumptions)
13. [Appendices](#13-appendices)

---

## 1. Introduction

### 1.1 Purpose of This FSD

This Functional Specification Document translates the requirements defined in PRD-001 (Platform Infrastructure & DevOps) into precise, implementable specifications. It documents the **behavior** of every infrastructure component -- not just its structure -- including all error states, state transitions, edge cases, and exact responses. A developer should be able to implement every feature described here using ONLY this FSD without referencing the PRD.

### 1.2 Scope

**This document covers:**

- Backend application scaffold (FastAPI on Railway) -- project structure, middleware behavior, error handling
- Frontend hosting configuration (Next.js on Vercel) -- deployment, headers, environment binding
- Database architecture (Supabase PostgreSQL) -- connection management, migrations, backup procedures
- Caching and message broker (Upstash Redis) -- key patterns, TTLs, cost constraints
- Task queue system (Celery workers) -- queue architecture, retry policies, circuit breaker behavior
- WebSocket real-time layer -- connection lifecycle, event contracts, heartbeat behavior
- CI/CD pipelines (GitHub Actions) -- pipeline stages, triggers, gates, notifications
- Monitoring, alerting, and observability -- tools, alert conditions, escalation paths
- Security posture -- authentication flows, rate limiting behavior, input validation, transport security
- Environment management (development/staging/production) -- configuration matrix, secret management
- Health check endpoints -- liveness, readiness, detailed diagnostics with exact response shapes

**This document does NOT cover:**

- Trendline detection algorithms [Cross-reference: see FSD-003 for trendline detection details]
- Broker adapter implementations [Cross-reference: see FSD-004 for broker adapter details]
- Trade execution logic [Cross-reference: see FSD-005 for trade execution details]
- Dashboard UI/UX design [Cross-reference: see FSD-007 for dashboard UI details]
- AI/ML model architecture [Cross-reference: see FSD-009 for AI/ML feature details]
- Data models and database schema definitions [Cross-reference: see FSD-002 for schema details]
- Notification system logic [Cross-reference: see FSD-008 for notification details]
- Billing and subscription management [Cross-reference: see FSD-010 for billing details]
- Analytics engine logic [Cross-reference: see FSD-011 for analytics details]

### 1.3 Relationship to Source PRD

This FSD is derived from PRD-001 v1.0 (2026-02-11). Every functional requirement (INF-FR-*), non-functional requirement (INF-NFR-*), security requirement (INF-SEC-*), DevOps requirement (INF-DEVOPS-*), and acceptance criterion (INF-AC-*) from the PRD is expanded here into implementable detail.

### 1.4 Cross-References to Other FSDs

| FSD | Relationship |
|---|---|
| FSD-002 | Depends on this FSD for database setup, migrations, connection pooling. This FSD provides the database infrastructure; FSD-002 defines the schema. |
| FSD-003 | Depends on this FSD for Celery workers, task retry policies. |
| FSD-004 | Depends on this FSD for environment config, secret management, circuit breaker infrastructure. |
| FSD-005 | Depends on this FSD for webhook endpoints, WebSocket layer, Redis pub/sub. |
| FSD-006 | Depends on this FSD for file storage (Supabase Storage), database transactions. |
| FSD-007 | Depends on this FSD for Vercel deployment, WebSocket client, CORS, API versioning. |
| FSD-008 | Depends on this FSD for Celery notification queue, Telegram bot configuration. |
| FSD-009 | Depends on this FSD for Celery low-priority queue, Claude API credentials. |
| FSD-010 | Depends on this FSD for Supabase Auth integration, environment configuration. |
| FSD-011 | Depends on this FSD for Redis caching, database query optimization infrastructure. |

### 1.5 Design Principles

These five principles govern every decision in this FSD:

1. **Cost-conscious by default.** Target $50-120/mo for MVP. Every service choice must justify its cost at current scale.
2. **Single-developer operable.** The entire stack must be manageable by one engineer. No Kubernetes. No self-managed databases. Managed services only.
3. **Paper-trade-safe.** Infrastructure must treat paper trading and live trading with identical reliability.
4. **Latency-aware, not latency-obsessed.** Target is 1H+ timeframe strategies. Sub-10-second webhook-to-order, not sub-second.
5. **Fail loud, fail safe.** Every failure must produce an alert. No silent data loss. No silent order failures. Circuit breakers over silent retries.

---

## 2. System Context

### 2.1 Where This Module Sits

TrendEdge follows a layered dependency model. This FSD defines Layer 1 -- the foundation that every other layer depends on:

```
Layer 4: User-Facing Features (FSD-007, FSD-008, FSD-010)
Layer 3: Domain Logic (FSD-003, FSD-004, FSD-005, FSD-006, FSD-009, FSD-011)
Layer 2: Data Layer (FSD-002)
Layer 1: Infrastructure (THIS FSD -- FSD-001)
```

No feature from Layers 2-4 can be implemented until the infrastructure defined here is operational and passing all acceptance criteria.

### 2.2 External Systems and Integrations

| External System | Direction | Protocol | Purpose | Criticality |
|---|---|---|---|---|
| Supabase (PostgreSQL 16) | Bidirectional | TCP (Supavisor port 6543) | Primary data store, Auth, Storage | Critical -- system non-functional without it |
| Upstash Redis | Bidirectional | TLS (rediss://) | Cache, Celery broker, pub/sub, rate limiting | Critical -- degraded mode possible |
| Railway | Outbound deploy | HTTPS | Backend hosting (API, Worker, Beat) | Critical -- system non-functional without it |
| Vercel | Outbound deploy | HTTPS | Frontend hosting | Critical -- UI non-functional without it |
| GitHub | Bidirectional | HTTPS | Source control, CI/CD triggers | Critical for development, not runtime |
| Sentry | Outbound | HTTPS | Error tracking and performance monitoring | High -- monitoring non-functional without it |
| Axiom | Outbound | HTTPS | Log aggregation and search | Medium -- can operate without it |
| Uptime Robot | Inbound (probes) | HTTPS | Health check monitoring | Medium -- alerting non-functional without it |
| Cloudflare | Proxy | HTTPS | DNS resolution, optional CDN | Low -- can bypass |
| TradingView | Inbound | HTTPS POST | Webhook signals | Critical for trade execution |
| Telegram | Outbound | HTTPS POST | Operator and user notifications | High for alerting |
| Interactive Brokers | Outbound | TCP/HTTPS | Trade execution [see FSD-004] | Critical for live trading |
| Tradovate | Outbound | HTTPS | Trade execution [see FSD-004] | Critical for live trading |

### 2.3 Context Diagram Description

The system has six primary communication paths:

1. **Browser to Frontend (Vercel):** User loads the Next.js application via Cloudflare DNS. Vercel serves static assets and server-rendered pages over HTTPS.
2. **Frontend to Backend (Railway):** The frontend calls the FastAPI REST API and establishes WebSocket connections over HTTPS/WSS. All requests include JWT authorization.
3. **Backend to Database (Supabase):** The API and workers query PostgreSQL via Supavisor connection pooler on port 6543 (transaction mode). Direct connections on port 5432 are used only for migrations.
4. **Backend to Redis (Upstash):** The API and workers use Redis for caching, Celery task brokering, pub/sub event distribution, rate limiting counters, and distributed locks. All connections use TLS.
5. **Backend to External Services:** Outbound calls to Sentry (error reports), Axiom (logs via Railway drain), Telegram (notifications), and broker APIs (trade execution).
6. **Inbound Webhooks:** TradingView sends trade signals as HTTPS POST requests to the webhook endpoint, authenticated via API key and HMAC signature.

### 2.4 Network Boundaries

```
Public Internet
    |
    +-- Vercel Edge Network
    |     - SSL termination
    |     - Static asset CDN
    |     - Server-side rendering
    |
    +-- Railway Public Endpoint
    |     - SSL termination
    |     - FastAPI application (REST + WebSocket)
    |     |
    |     +-- Railway Internal Network
    |           - API <-> Workers communicate via Redis only (no direct connections)
    |           - Workers share same internal network but are isolated processes
    |
    +-- Supabase Cloud
    |     - Supavisor connection pooler (port 6543, transaction mode)
    |     - Direct connection (port 5432, migrations only)
    |     - Supabase Auth (HTTPS API)
    |     - Supabase Storage (HTTPS/S3 API)
    |
    +-- Upstash Redis
          - TLS-encrypted endpoint (rediss://)
          - Serverless, pay-per-command
```

**Security boundary rules:**
- No service exposes a port directly to the public internet except through its hosting platform's SSL termination.
- All inter-service communication uses encrypted transports (TLS/SSL).
- Database direct connections (port 5432) are restricted to CI/CD migration jobs and local development.
- Redis connections reject plaintext `redis://` -- only `rediss://` (TLS) is accepted in staging and production.

---

## 3. Functional Specifications

### 3.1 FastAPI Application Scaffold

#### 3.1.1 Project Structure (Source: INF-FR-001)

**Description:** The backend application follows a layered architecture with clear separation of concerns. The directory structure enforces boundaries between API routing, business logic, data access, and background tasks.

**Required Directory Structure:**

```
backend/
|-- app/
|   |-- __init__.py
|   |-- main.py                  # FastAPI app factory, lifespan events
|   |-- config.py                # Pydantic Settings (env-based config)
|   |-- dependencies.py          # Shared FastAPI dependencies (db, redis, auth)
|   |
|   |-- api/
|   |   |-- __init__.py
|   |   |-- v1/
|   |   |   |-- __init__.py
|   |   |   |-- router.py        # Aggregated v1 router
|   |   |   |-- health.py        # Health check endpoints
|   |   |   |-- webhooks.py      # TradingView webhook receiver
|   |   |   |-- trades.py        # Trade CRUD
|   |   |   |-- trendlines.py    # Trendline CRUD + alerts
|   |   |   |-- playbooks.py     # Playbook management
|   |   |   |-- analytics.py     # Analytics queries
|   |   |   |-- journal.py       # Journal entries
|   |   |   |-- users.py         # User profile & settings
|   |   |   |-- ws.py            # WebSocket endpoints
|   |
|   |-- core/
|   |   |-- __init__.py
|   |   |-- security.py          # JWT validation, API key verification
|   |   |-- middleware.py         # CORS, logging, timing, rate limit
|   |   |-- exceptions.py        # Custom exception classes + handlers
|   |   |-- logging.py           # Structured logging configuration
|   |
|   |-- db/
|   |   |-- __init__.py
|   |   |-- session.py           # Async SQLAlchemy session factory
|   |   |-- base.py              # Declarative base
|   |   |-- models/              # SQLAlchemy ORM models
|   |   |-- repositories/        # Data access layer (repository pattern)
|   |
|   |-- schemas/
|   |   |-- __init__.py
|   |   |-- requests/            # Pydantic request models
|   |   |-- responses/           # Pydantic response models
|   |
|   |-- services/
|   |   |-- __init__.py
|   |   |-- trade_service.py
|   |   |-- trendline_service.py
|   |   |-- notification_service.py
|   |
|   |-- tasks/
|   |   |-- __init__.py
|   |   |-- celery_app.py        # Celery application factory
|   |   |-- trendline_tasks.py
|   |   |-- notification_tasks.py
|   |   |-- analytics_tasks.py
|   |
|   |-- adapters/
|   |   |-- __init__.py
|   |   |-- broker_base.py       # Abstract BrokerInterface
|   |   |-- ibkr_adapter.py
|   |   |-- tradovate_adapter.py
|   |
|   |-- utils/
|       |-- __init__.py
|       |-- symbols.py           # Continuous symbol mapping
|       |-- time.py              # Session time helpers, timezone utils
|
|-- alembic/
|   |-- env.py
|   |-- versions/                # Migration files
|
|-- tests/
|   |-- conftest.py              # Fixtures: test db, client, auth
|   |-- unit/
|   |-- integration/
|   |-- e2e/
|
|-- alembic.ini
|-- pyproject.toml
|-- Dockerfile
|-- docker-compose.yml
|-- Makefile
|-- .env.example
```

**Behavioral Requirements:**

| Command | Expected Behavior | Success Condition | Failure Behavior |
|---|---|---|---|
| `make dev` | Starts full local stack (PostgreSQL, Redis, API, Worker, Beat) via `docker compose up` | All 5 services report healthy within 60 seconds. `curl http://localhost:8000/health` returns 200. | If any service fails to start, Docker logs are printed to stdout. Exit code 1. |
| `make test` | Runs the full test suite (`pytest tests/ -v --cov=app`) | All tests pass. Coverage report printed. Exit code 0. | Failed tests are listed with tracebacks. Exit code 1. |
| `make lint` | Runs `ruff check .` and `ruff format --check .` and `mypy app/` | Zero lint errors, zero format violations, zero type errors. Exit code 0. | Each violation printed with file, line, and rule. Exit code 1. |
| `make migrate` | Runs `alembic upgrade head` against the local database | All pending migrations applied. "Done" printed. Exit code 0. | Migration error printed with SQL statement that failed. Exit code 1. Partial migrations are rolled back. |
| `make seed` | Runs `python -m app.scripts.seed` to populate test data | Database populated with realistic test data (users, trades, trendlines). Count of records printed. | If seed fails, error message printed. Exit code 1. Previously seeded data is not duplicated (idempotent). |

**Edge Cases:**
- If `make dev` is run when containers are already running, it should detect existing containers and restart only changed services (default `docker compose up` behavior).
- If `make test` is run without the database or Redis, tests that require them should skip with a clear message ("Skipping: PostgreSQL not available"), not fail with connection errors.
- If `make migrate` is run with no pending migrations, it should print "No migrations to apply" and exit 0 (not error).

**What This Must NOT Do:**
- The project structure must NOT include any business logic in the `api/` layer. Route handlers must delegate to `services/`.
- The `db/repositories/` layer must NOT import from `api/` or `services/`. Data access flows one direction only.
- No file in `app/` may import directly from `tests/`.

#### 3.1.2 Application Factory and Lifespan (Source: INF-FR-001, main.py)

**Description:** The FastAPI application uses a factory pattern with lifespan context manager for startup/shutdown events.

**Startup Behavior (in order):**

1. Load configuration from environment variables via Pydantic Settings.
2. Validate all required settings are present. If any required setting is missing (DATABASE_URL, UPSTASH_REDIS_URL, SUPABASE_URL, SUPABASE_ANON_KEY, SUPABASE_SERVICE_ROLE_KEY, SUPABASE_JWT_SECRET), log a CRITICAL error with the missing variable name and exit with code 1. Exact log message: `"CRITICAL: Required environment variable '{VAR_NAME}' is not set. Application cannot start."`
3. Initialize async SQLAlchemy engine and verify database connectivity with a `SELECT 1` query. If the database is unreachable after 3 attempts (1 second apart), log: `"CRITICAL: Cannot connect to database after 3 attempts. Last error: {error_message}"` and exit with code 1.
4. Initialize Redis client and verify connectivity with a `PING` command. If Redis is unreachable after 3 attempts (1 second apart), log: `"WARNING: Cannot connect to Redis. Starting in degraded mode (no caching, synchronous task processing)."` The application MUST still start.
5. Initialize Sentry SDK (if SENTRY_DSN is set). If SENTRY_DSN is empty, log: `"INFO: Sentry DSN not configured. Error tracking disabled."`
6. Register all middleware in the correct order (see 3.1.3).
7. Include API routers.
8. Log: `"INFO: TrendEdge API v{version} started successfully in {APP_ENV} mode."`

**Shutdown Behavior (in order):**

1. Log: `"INFO: TrendEdge API shutting down..."`
2. Close all WebSocket connections with close code 1001 (Going Away) and message "Server shutting down".
3. Dispose of the SQLAlchemy engine (closes all database connections).
4. Close the Redis client connection pool.
5. Log: `"INFO: TrendEdge API shutdown complete."`

**Degraded Mode Behavior (Redis unavailable):**

When Redis is unavailable at startup or becomes unavailable during operation:
- Caching operations silently return cache misses (no errors to callers).
- Rate limiting is disabled (all requests pass). Log: `"WARNING: Rate limiting disabled -- Redis unavailable."`
- Celery tasks for critical operations (webhook processing) are executed synchronously in the API process. Log: `"WARNING: Executing task {task_name} synchronously -- Celery broker unavailable."`
- WebSocket pub/sub is non-functional. Clients receive no real-time updates. The WebSocket connection remains open but idle.
- The `/health/ready` endpoint returns HTTP 503 with `"redis": {"status": "error", "message": "Connection refused"}`.

#### 3.1.3 Middleware Stack (Source: INF-FR-002)

**Description:** Middleware is applied in a specific order. The outermost middleware processes first on request and last on response.

**Middleware Order (outermost first):**

| Order | Middleware | Behavior on Request | Behavior on Response | Error Behavior |
|---|---|---|---|---|
| 1 | `RequestIDMiddleware` | If `X-Request-ID` header is present and is a valid UUID, use it. Otherwise, generate a new UUID v4. Store in request state. | Add `X-Request-ID` header to response with the request ID. | Never fails. Always generates a fallback ID. |
| 2 | `TimingMiddleware` | Record `time.perf_counter()` at request start. | Calculate `duration_ms = (end - start) * 1000`. Add to log context. If duration > 5000ms, log WARNING: `"Slow request: {method} {path} took {duration_ms}ms"`. | Never fails. If timing calculation errors, log 0ms. |
| 3 | `CORSMiddleware` | Validate `Origin` header against allowed origins list. For preflight (OPTIONS), return 200 with CORS headers immediately. | Add CORS headers (`Access-Control-Allow-Origin`, etc.) to response. | If origin not in allowed list, omit CORS headers (browser blocks request). Do NOT return an error response -- just omit headers. |
| 4 | `RateLimitMiddleware` | Check Redis for rate limit counter. If limit exceeded, return HTTP 429 immediately (do not call endpoint handler). | Add `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset` headers. | If Redis is unavailable, allow request through (fail open). Log: `"WARNING: Rate limit check skipped -- Redis unavailable."` |
| 5 | `RequestLoggingMiddleware` | Log request start at DEBUG level: `"Request started: {method} {path}"`. | Log request completion at INFO level with all structured fields (see 3.1.7). | Log errors at ERROR level with exception details. Never swallow exceptions. |
| 6 | `SentryMiddleware` | Attach request context (request_id, user_id, path) to Sentry scope. | No action. | Capture unhandled exceptions and send to Sentry with full context. Re-raise exception for error handler. |

**State Transitions for Request Processing:**

```
Request Received
    |
    v
[RequestIDMiddleware] -- assigns/extracts request_id
    |
    v
[TimingMiddleware] -- starts timer
    |
    v
[CORSMiddleware] -- if OPTIONS: return 200 with headers (skip remaining)
    |                if disallowed origin: continue but omit CORS headers on response
    v
[RateLimitMiddleware] -- if limit exceeded: return 429 (skip remaining)
    |                     if Redis down: continue (fail open)
    v
[RequestLoggingMiddleware] -- log request start
    |
    v
[SentryMiddleware] -- attach context
    |
    v
[Route Handler] -- process request
    |
    v
(Response flows back through middleware in reverse order)
```

#### 3.1.4 Error Handling (Source: INF-FR-003)

**Description:** All API errors return a consistent JSON structure. No error response may deviate from this format. Internal details must never leak to the client.

**Standard Error Response Format:**

```json
{
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Human-readable description",
    "details": [
      {
        "field": "entry_price",
        "message": "Must be a positive number"
      }
    ],
    "request_id": "550e8400-e29b-41d4-a716-446655440000",
    "timestamp": "2026-02-11T10:30:00Z"
  }
}
```

**Field Specifications:**

| Field | Type | Required | Constraints | Description |
|---|---|---|---|---|
| `error.code` | string | Yes | Must be one of the defined error codes below | Machine-readable error identifier |
| `error.message` | string | Yes | Max 500 characters. Must not contain stack traces, file paths, SQL, or internal identifiers. | Human-readable error description |
| `error.details` | array | No | Each element has `field` (string) and `message` (string). Max 50 items. | Field-level validation errors |
| `error.request_id` | string (UUID) | Yes | UUID v4 format | Request correlation ID from middleware |
| `error.timestamp` | string (ISO 8601) | Yes | UTC timezone, format `YYYY-MM-DDTHH:MM:SSZ` | Time the error occurred |

**Error Code Registry:**

| HTTP Status | Error Code | When Used | Example Message |
|---|---|---|---|
| 400 | `VALIDATION_ERROR` | Pydantic validation fails on request body or query params | "Request validation failed" |
| 401 | `AUTHENTICATION_REQUIRED` | No Authorization header, expired JWT, invalid JWT signature | "Authentication required. Please provide a valid access token." |
| 403 | `FORBIDDEN` | Valid JWT but user lacks permission for the resource | "You do not have permission to access this resource." |
| 404 | `NOT_FOUND` | Requested resource does not exist or user does not own it | "Trade with ID '123' not found." |
| 409 | `CONFLICT` | Duplicate resource creation or invalid state transition | "A trade with this external ID already exists." |
| 422 | `UNPROCESSABLE_ENTITY` | Valid syntax but semantically invalid (e.g., invalid futures symbol) | "Symbol 'INVALID' is not a recognized futures instrument." |
| 429 | `RATE_LIMITED` | Rate limit exceeded | "Rate limit exceeded. Try again in {seconds} seconds." |
| 500 | `INTERNAL_ERROR` | Unhandled server error | "An internal error occurred. This has been reported automatically." |
| 502 | `BROKER_ERROR` | Upstream broker API returned error or timed out | "Broker service is temporarily unavailable. Please try again." |
| 503 | `SERVICE_UNAVAILABLE` | A critical dependency (DB, Redis) is down | "Service temporarily unavailable. Please try again in a few minutes." |

**Error Handling Behavior by Exception Type:**

| Exception Source | Behavior | Logged To | Client Response |
|---|---|---|---|
| Pydantic `ValidationError` | Catch and transform to 400 response with field-level details | INFO level (expected) | 400 with details array listing each invalid field |
| Custom `AuthenticationError` | Return 401 | WARNING level | 401 with generic auth message |
| Custom `ForbiddenError` | Return 403 | WARNING level | 403 with generic permission message |
| Custom `NotFoundError` | Return 404 | DEBUG level (expected) | 404 with resource type and ID |
| Custom `ConflictError` | Return 409 | INFO level | 409 with conflict description |
| Custom `BrokerError` | Return 502 | ERROR level with broker response details | 502 with generic broker message |
| `asyncpg.PostgresError` | Return 503 | ERROR level with query details (redacted) | 503 with generic unavailable message |
| `redis.ConnectionError` | Return 503 or degrade gracefully (see degraded mode) | ERROR level | 503 or degraded response |
| Unhandled `Exception` | Return 500 | CRITICAL level + Sentry capture with full traceback | 500 with generic message. NO stack trace. NO file paths. NO SQL. |

**Negative Scenarios (What Must NOT Happen):**
- 5xx error responses must NEVER contain: stack traces, file paths, database table names, SQL queries, internal IP addresses, or environment variable values.
- Error responses must NEVER omit the `request_id` field -- it is always present.
- A `VALIDATION_ERROR` must NEVER be returned for authentication failures (use `AUTHENTICATION_REQUIRED`).
- Multiple errors in a single request (e.g., 3 invalid fields) must all be returned in one response, not one at a time.

#### 3.1.5 API Versioning (Source: INF-FR-016)

**Description:** All API endpoints are prefixed with a version path segment. Only URL path versioning is used.

**Behavior:**

- Current version: `v1`. All endpoints live under `/api/v1/`.
- Requests to `/api/` (no version) return HTTP 404 with message: `"API version required. Use /api/v1/ prefix."`
- Requests to `/api/v99/` (non-existent version) return HTTP 404 with message: `"API version 'v99' is not supported. Available versions: v1."`
- Every response includes the header `X-API-Version: v1`.
- Health check endpoints (`/health`, `/health/ready`, `/health/detailed`) are NOT versioned -- they live at the root.

**Breaking Change Definition:**
- Removing a field from a response
- Changing a field's type
- Removing an endpoint
- Changing authentication requirements for an endpoint
- Renaming a field

**Non-Breaking Changes (do not require new version):**
- Adding optional fields to request/response
- Adding new endpoints
- Adding new enum values
- Adding optional query parameters

**Deprecation Policy:** Minimum 6 months notice before removing a version. Deprecated endpoints return `Deprecation: true` and `Sunset: {date}` headers.

#### 3.1.6 CORS Configuration (Source: INF-FR-017)

**Description:** Cross-Origin Resource Sharing is configured per environment to allow the frontend to call the API.

**Allowed Origins by Environment:**

| Environment | Allowed Origins |
|---|---|
| Development | `http://localhost:3000`, `http://127.0.0.1:3000` |
| Staging | `https://staging.trendedge.app` |
| Production | `https://app.trendedge.app`, `https://trendedge.app` |

**CORS Headers:**

| Header | Value |
|---|---|
| `Access-Control-Allow-Credentials` | `true` |
| `Access-Control-Allow-Methods` | `GET, POST, PUT, PATCH, DELETE, OPTIONS` |
| `Access-Control-Allow-Headers` | `Authorization, Content-Type, X-Request-ID, X-API-Key` |
| `Access-Control-Expose-Headers` | `X-Request-ID, X-API-Version, X-RateLimit-Remaining` |
| `Access-Control-Max-Age` | `600` (preflight cache: 10 minutes) |

**Behavior:**
- Preflight (OPTIONS) requests return 200 immediately with CORS headers. They do NOT hit rate limiting or authentication.
- Requests from unlisted origins receive no CORS headers. The browser will block the response. The server does NOT return an error -- it simply omits the `Access-Control-Allow-Origin` header.
- The `CORS_ORIGINS` setting is a list of strings loaded from the environment variable. In development, multiple origins are comma-separated in the env var: `CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000`.

#### 3.1.7 Request/Response Logging (Source: INF-FR-018)

**Description:** All log entries are structured JSON. Plaintext logging is not permitted.

**Required Log Fields for Every Request:**

```json
{
  "timestamp": "2026-02-11T10:30:00.123Z",
  "level": "INFO",
  "logger": "trendedge.api",
  "message": "Request completed",
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "method": "POST",
  "path": "/api/v1/webhooks/tradingview",
  "status_code": 200,
  "duration_ms": 45,
  "user_id": "user_abc123",
  "ip": "203.0.113.42",
  "user_agent": "TradingView/1.0"
}
```

**Log Level Usage:**

| Level | When to Use | Example |
|---|---|---|
| `DEBUG` | Detailed diagnostic info, only in development | "Parsing webhook payload: {redacted_fields}" |
| `INFO` | Request lifecycle, business events | "Request completed", "Trade created: {trade_id}", "Migration applied: {name}" |
| `WARNING` | Recoverable issues | "Rate limit approaching for user {user_id}", "Redis connection retry 2/3", "Slow request: 5200ms" |
| `ERROR` | Failures requiring attention | "Database query failed: {redacted_error}", "Broker API returned 500", "Celery task failed: {task_id}" |
| `CRITICAL` | System-level failures | "Cannot connect to database", "Required env var missing", "All Celery workers down" |

**Data Sensitivity Rules:**

| Data Type | Logging Rule |
|---|---|
| Passwords, API keys, JWT tokens, broker credentials | NEVER log. Not even partially. |
| Credit card numbers | NEVER log. |
| User email, full name | Do NOT log in standard request logs. Allowed in auth-specific audit logs only. |
| User ID (UUID) | Always log (opaque, non-PII). |
| IP address | Allowed in access logs only. 30-day retention. |
| Request bodies (webhooks) | Log with secrets redacted. Replace any field containing "key", "secret", "token", "password" with "[REDACTED]". |
| Request bodies (user data endpoints) | Do NOT log. |
| Response bodies | Do NOT log. Log status code and response size in bytes only. |

**Log Destination:**
- All services write to stdout.
- Railway captures stdout and forwards to Axiom via Railway's native log drain integration.
- Vercel logs are forwarded via Axiom's Vercel integration.
- Retention: 30 days (Axiom free tier limit).

### 3.2 Database Setup (Supabase PostgreSQL)

#### 3.2.1 Connection Management (Source: INF-FR-004)

**Description:** The application connects to Supabase PostgreSQL through the Supavisor connection pooler for all runtime queries. Direct connections are used only for migrations.

**Connection Configuration:**

| Parameter | Value | Rationale |
|---|---|---|
| Runtime connection | Supavisor, port 6543, transaction mode | Connection pooling for application queries |
| Migration connection | Direct, port 5432 | Alembic requires direct connections for DDL |
| API pool size | 10 connections | Adequate for single-instance MVP |
| API max overflow | 5 connections | Burst capacity |
| Worker pool size | 5 connections | Workers need fewer concurrent queries |
| Worker max overflow | 3 connections | Burst capacity |
| `pool_pre_ping` | `True` | Detect stale connections before use |
| `pool_recycle` | 300 seconds | Reclaim idle connections after 5 minutes |
| `statement_cache_size` | 0 | Required for Supavisor compatibility (prepared statements not supported in transaction mode) |
| Statement timeout | 30 seconds | Prevent runaway queries |
| Connection SSL mode | `require` | Enforce encrypted connections |

**Connection Initialization Code:**

```python
engine = create_async_engine(
    settings.DATABASE_URL,
    pool_size=settings.DATABASE_POOL_SIZE,
    max_overflow=settings.DATABASE_MAX_OVERFLOW,
    pool_pre_ping=True,
    pool_recycle=300,
    connect_args={
        "statement_cache_size": 0,
        "command_timeout": 30,
    },
)
```

**Connection Error Behavior:**

| Scenario | Detection | Behavior | Recovery |
|---|---|---|---|
| Database unreachable at startup | `SELECT 1` fails 3 times | Application exits with code 1. Log CRITICAL. | Operator restarts after fixing connectivity. |
| Connection lost during request | `asyncpg.ConnectionDoesNotExistError` | Return HTTP 503 to client. Log ERROR. | SQLAlchemy pool_pre_ping detects on next request and reconnects. |
| Pool exhausted | All connections in use, overflow exceeded | Request waits up to 30 seconds (pool timeout). If still exhausted, return HTTP 503. Log ERROR: `"Database connection pool exhausted. Pool size: {size}, overflow: {overflow}, checked out: {checked_out}."` | Connections are returned to pool when requests complete. Alert fires if >80% utilization sustained. |
| Supavisor restart | Connections drop | pool_pre_ping detects stale connections. Next request gets fresh connection. | Automatic via pool_pre_ping. Brief latency spike possible. |

**Edge Cases:**
- If the `DATABASE_URL` uses port 5432 instead of 6543 in production, log a WARNING: `"Using direct database connection (port 5432) in production. This may exhaust Supabase connection limits. Use Supavisor (port 6543)."` The application still starts.
- If `statement_cache_size` is not set to 0, Supavisor will return intermittent errors. The configuration MUST enforce this.

#### 3.2.2 Migration Strategy (Source: INF-FR-005)

**Description:** Database schema changes are managed through Alembic migrations with strict safety rules.

**Migration File Naming:** `YYYY_MM_DD_HHMM_<description>.py` (e.g., `2026_02_15_1430_create_trades_table.py`)

**Migration Workflow:**

```
State: Developer wants to change schema
    |
    v
[1] Modify SQLAlchemy model in app/db/models/
    |
    v
[2] Generate migration: alembic revision --autogenerate -m "description"
    |
    v
[3] Review generated migration file:
    |   - Verify upgrade() contains correct DDL
    |   - Verify downgrade() fully reverses upgrade()
    |   - Check for data-destructive operations
    |
    v
[4] Test locally:
    |   alembic upgrade head     (apply)
    |   alembic downgrade -1     (reverse)
    |   alembic upgrade head     (re-apply)
    |   All three must succeed.
    |
    v
[5] Commit migration + model changes in same PR
    |
    v
[6] CI runs migrations against ephemeral test database
    |
    v
[7] Merge to main -> CD applies to staging
    |
    v
[8] Verify staging is healthy (health check + smoke test)
    |
    v
[9] CD applies to production (with approval gate in Phase 2+)
```

**Migration Safety Rules:**

1. **No DROP TABLE or DROP COLUMN** without a prior deprecation migration. To remove a column: first deploy a migration that stops writing to it, then in a subsequent release add the DROP.
2. **All ALTER TABLE operations must be backward-compatible.** Add nullable columns. Do NOT rename columns (add new + deprecate old).
3. **Large data migrations** (affecting >10,000 rows) must run as background Celery tasks, NOT in the Alembic migration. The migration should add schema changes only.
4. **Each migration must complete in < 30 seconds** on production data. Migrations that might exceed this (adding indexes on large tables) must use `CREATE INDEX CONCURRENTLY`.
5. **Only one migration per PR.** If a PR has conflicting migrations, they must be resolved (re-generated) before merge.

**Migration Error Behavior:**

| Scenario | Behavior |
|---|---|
| Migration fails in CI | PR build fails. Developer must fix migration. |
| Migration fails in staging | Deployment halts. Staging is left in failed state. Alert sent to Telegram. Developer must create a fix migration. |
| Migration fails in production | Deployment halts. Production database is in partially migrated state. CRITICAL alert to Telegram + Email. Manual intervention required. |
| `downgrade()` is empty or raises `NotImplementedError` | CI MUST reject the migration. Every migration MUST have a working downgrade. |

#### 3.2.3 Database Backup Strategy (Source: INF-FR-006)

**Description:** Backups protect against data loss. The strategy uses Supabase built-in backups supplemented by custom `pg_dump` backups.

**Backup Tiers:**

| Tier | Method | Frequency | Retention | RPO | RTO |
|---|---|---|---|---|---|
| Supabase Free | Supabase daily backups | Daily (automated) | 7 days | 24 hours | 1 hour |
| Supabase Pro ($25/mo) | Point-in-time recovery (WAL) | Continuous | 7 days | Minutes | 30 minutes |
| Custom supplement | `pg_dump` via GitHub Actions cron | Daily at 02:00 UTC | 30 days (Cloudflare R2 or Supabase Storage) | 24 hours | 2 hours |

**Phase 1 Implementation (Custom pg_dump):**

- A GitHub Actions workflow runs daily at 02:00 UTC.
- It connects to Supabase via the direct connection (port 5432) using a CI-specific database credential.
- Runs `pg_dump --format=custom --compress=9` to produce a compressed backup.
- Uploads the backup to Cloudflare R2 (or Supabase Storage) with key pattern: `backups/trendedge_{YYYY-MM-DD}.dump`.
- Deletes backups older than 30 days.
- On success: logs INFO. On failure: sends Telegram alert to operator with error message.

**Backup Verification:**
- Monthly: restore a backup to a temporary database and verify row counts match expected ranges.
- Log the verification result. If row count deviates by >10% from the previous backup, send a WARNING alert.

**Recovery Procedure:**

1. Identify the backup to restore (daily backup closest to desired point).
2. Create a new Supabase project or use a temporary database.
3. Run `pg_restore --format=custom backup_file.dump`.
4. Verify data integrity (row counts, latest trade timestamps).
5. Update application DATABASE_URL to point to restored database.
6. Restart all services.
7. Verify health checks pass.

### 3.3 Redis Configuration (Upstash)

#### 3.3.1 Redis Usage Patterns (Source: INF-FR-007)

**Description:** Redis serves five distinct purposes. Each has specific key patterns, TTLs, and behavioral requirements.

**Key Pattern Registry:**

| Usage | Key Pattern | TTL | Serialization | Example | Max Size |
|---|---|---|---|---|---|
| API response cache | `cache:{endpoint}:{user_id}:{param_hash}` | 60-300s (endpoint-dependent) | JSON | `cache:analytics:user123:a1b2c3` | 1 MB per key |
| Rate limiting | `ratelimit:{category}:{identifier}` | Equal to rate limit window | Integer counter | `ratelimit:webhook:apikey_abc` | 8 bytes |
| Celery broker | `celery` (managed by Celery) | Task-dependent | JSON | Celery default keys | Celery managed |
| Celery result backend | `celery-task-meta-{task_id}` | 3600s (1 hour) | JSON | `celery-task-meta-abc123` | 64 KB |
| WebSocket pub/sub | `ws:{channel}:{user_id}` | N/A (pub/sub, no persistence) | JSON | `ws:trades:user123` | 64 KB per message |
| Webhook dedup | `dedup:webhook:{payload_hash}` | 60s | String (empty value) | `dedup:webhook:sha256_abc` | 1 byte |
| Distributed locks | `lock:{resource}` | 30s (auto-expire) | String (lock holder ID) | `lock:trendline_scan:NQ` | 64 bytes |

**Cache Invalidation Behavior:**

| Event | Cache Keys Invalidated | Method |
|---|---|---|
| Trade created/updated/deleted | `cache:trades:*:{user_id}:*`, `cache:analytics:*:{user_id}:*` | Explicit DELETE |
| Trendline alert triggered | `cache:trendlines:*:{user_id}:*` | Explicit DELETE |
| User settings changed | `cache:users:{user_id}:*` | Explicit DELETE |
| Cache miss | N/A | Fetch from DB, populate cache with TTL |

**Cost Management (Upstash charges per command):**

- Free tier: 10,000 commands/day.
- Target: < 10,000 commands/day for MVP.
- PROHIBITED patterns: polling loops, `KEYS *` scans, `SCAN` iterations over large keyspaces.
- REQUIRED patterns: pub/sub for real-time (single subscribe command per connection), batch operations where possible (`MGET`, `MSET`), pipeline commands.

#### 3.3.2 Redis Connection Configuration (Source: INF-FR-008)

**Connection Parameters:**

```python
redis_client = Redis.from_url(
    settings.UPSTASH_REDIS_URL,   # rediss://default:xxx@us1-xxx.upstash.io:6379
    decode_responses=True,
    socket_timeout=5,              # 5 second read timeout
    socket_connect_timeout=5,      # 5 second connect timeout
    retry_on_timeout=True,         # Auto-retry on timeout
    max_connections=20,            # Connection pool size
)
```

**Connection Error Behavior:**

| Scenario | Behavior | Log Message |
|---|---|---|
| Redis unreachable at startup | Application starts in degraded mode | `"WARNING: Redis unavailable. Starting in degraded mode."` |
| Redis unreachable during request | Cache operations return None (miss). Rate limiting skipped. | `"WARNING: Redis operation failed: {operation}. Error: {error}"` |
| Redis timeout (>5s) | Retry once. If second attempt times out, treat as unavailable. | `"ERROR: Redis timeout on {operation}. Retried once. Falling back."` |
| Redis connection pool exhausted | Wait up to 5 seconds for a connection. If unavailable, treat as Redis unavailable. | `"ERROR: Redis connection pool exhausted. Max connections: 20."` |

### 3.4 Celery Worker Configuration

#### 3.4.1 Queue Architecture (Source: INF-FR-009)

**Description:** Tasks are distributed across four logical queues with different priorities and purposes.

**Queue Definitions:**

| Queue | Concurrency (MVP) | Purpose | Task Examples |
|---|---|---|---|
| `high` | 2 (prefork) | Trade execution, order management, webhook processing | `process_webhook`, `submit_order`, `update_order_status` |
| `default` | 4 (prefork) | Trendline scans, alert evaluation, analytics | `run_trendline_scan`, `evaluate_alerts`, `update_analytics` |
| `low` | 2 (prefork) | AI analysis, report generation, data backfill | `ai_trade_analysis`, `generate_report`, `backfill_data` |
| `notifications` | 2 (prefork) | Telegram, Discord, email dispatch | `send_telegram`, `send_discord`, `send_email` |

**MVP Simplification:** A single worker process handles all four queues with combined concurrency of 4. Command: `celery -A app.tasks.celery_app worker -Q high,default,low,notifications -c 4`. Queue separation is logical, enabling future scaling by running dedicated workers per queue.

**Task Priority Behavior:**
- The `high` queue is listed first in the `-Q` argument, so Celery processes `high` tasks before `default`, `default` before `low`, etc.
- If the `high` queue has pending tasks, `low` queue tasks wait until `high` is drained.

#### 3.4.2 Celery Configuration (Source: INF-FR-010)

**Configuration Values:**

| Setting | Value | Purpose |
|---|---|---|
| `broker_url` | `settings.UPSTASH_REDIS_URL` | Redis as message broker |
| `result_backend` | `settings.UPSTASH_REDIS_URL` | Store task results in Redis |
| `task_serializer` | `"json"` | JSON serialization (no pickle for security) |
| `result_serializer` | `"json"` | JSON serialization for results |
| `accept_content` | `["json"]` | Only accept JSON (reject pickle) |
| `timezone` | `"UTC"` | All timestamps in UTC |
| `enable_utc` | `True` | Force UTC |
| `task_track_started` | `True` | Track when tasks begin executing |
| `task_acks_late` | `True` | Acknowledge after completion (re-deliver on crash) |
| `worker_prefetch_multiplier` | `1` | Fetch one task at a time (fair scheduling) |
| `task_reject_on_worker_lost` | `True` | Reject task if worker dies (triggers redelivery) |
| `result_expires` | `3600` | Task results expire after 1 hour |
| `worker_max_tasks_per_child` | `200` | Restart child process after 200 tasks (prevent memory leaks) |
| `worker_max_memory_per_child` | `512000` | Kill child if it exceeds 512 MB |
| `broker_transport_options.visibility_timeout` | `600` | 10 minute visibility timeout for long tasks |

**What Must NOT Happen:**
- `accept_content` must NEVER include `"pickle"`. Pickle deserialization is a remote code execution vector.
- `task_acks_late` must be `True`. If set to `False`, a crash during task execution causes silent task loss.
- `task_serializer` must be `"json"`. All task arguments must be JSON-serializable (no Python objects, no datetime objects -- use ISO strings).

#### 3.4.3 Task Retry Policies (Source: INF-FR-011)

**Retry Configuration by Task Category:**

| Category | Max Retries | Backoff Strategy | Delays | On Final Failure |
|---|---|---|---|---|
| Webhook processing | 3 | Exponential | 5s, 25s, 125s | Log failed webhook to DB (`failed_webhooks` table) with payload. Send Sentry alert. |
| Broker API calls | 5 | Exponential + jitter | ~2s, ~8s, ~32s, ~128s, ~512s | Trigger circuit breaker (see below). Log to DB. Send Telegram CRITICAL alert. |
| Trendline scan | 2 | Fixed | 60s, 60s | Log failure. Task will be retried on next scheduled scan. No alert (transient). |
| Notification dispatch | 3 | Exponential | 10s, 60s, 300s | Log to DB (`failed_notifications` table). Skip this notification. No retry. |
| AI/ML tasks | 1 | None | N/A | Log failure. Manual retry via admin action. |

**Circuit Breaker Specification:**

The circuit breaker pattern protects against cascading failures when a broker API is consistently failing.

**State Machine:**

| State | Behavior | Transition |
|---|---|---|
| CLOSED (normal) | All broker API tasks execute normally. | If 3 consecutive failures within 5 minutes -> open circuit. |
| OPEN (tripped) | All broker API tasks for the affected broker are immediately rejected without calling the broker. Tasks return error: `"Circuit breaker open for {broker_name}. Retry after {resume_time}."` | After 10 minutes -> HALF_OPEN. |
| HALF_OPEN (testing) | Allow 1 task through to test if broker is recovered. | If task succeeds -> CLOSED. If task fails -> OPEN (reset 10-minute timer). |

**Circuit Breaker State Storage:** Redis key `circuit:{broker_name}` with value `{"state": "OPEN", "opened_at": "2026-02-11T10:30:00Z", "failure_count": 3}` and TTL of 15 minutes (auto-close if Redis loses the key).

**Circuit Breaker Notifications:**
- On OPEN: Send Telegram alert: `"CRITICAL: Circuit breaker OPENED for {broker_name}. {failure_count} consecutive failures. All execution tasks halted for 10 minutes."`
- On CLOSED (recovery): Send Telegram alert: `"RESOLVED: Circuit breaker CLOSED for {broker_name}. Broker API recovered. Execution tasks resumed."`

### 3.5 WebSocket Support

#### 3.5.1 WebSocket Architecture (Source: INF-FR-012)

**Description:** WebSocket connections provide real-time event delivery from the server to connected browser clients. Events originate from API handlers, Celery workers, and broker adapters, are published to Redis pub/sub, and are fanned out to connected WebSocket clients.

**Connection Lifecycle:**

```
State: Client wants real-time updates
    |
    v
[1] Client opens WSS connection to /api/v1/ws
    |   - Client sends JWT as query parameter: /api/v1/ws?token={jwt}
    |
    v
[2] Server validates JWT:
    |   - Valid -> accept connection, extract user_id
    |   - Expired -> reject with close code 4001, reason "Token expired"
    |   - Invalid signature -> reject with close code 4003, reason "Invalid token"
    |   - Missing -> reject with close code 4000, reason "Authentication required"
    |
    v
[3] Server checks connection count for user:
    |   - If < 10 connections -> accept
    |   - If >= 10 connections -> reject with close code 4009, reason "Maximum connections exceeded"
    |
    v
[4] Server subscribes to user's Redis pub/sub channels:
    |   - ws:trades:{user_id}
    |   - ws:trendlines:{user_id}
    |   - ws:system:{user_id}
    |
    v
[5] Connection is active. Server sends events as they arrive.
    |
    +-- Every 30 seconds: server sends ping frame
    |   |   - Client must respond with pong within 10 seconds
    |   |   - If no pong received -> close connection (code 1001, "Heartbeat timeout")
    |
    +-- Every 15 minutes: server re-validates JWT
    |   |   - If JWT now expired -> send close frame (code 4001, "Token expired")
    |   |   - Client should refresh token and reconnect
    |
    v
[6] Connection closes:
    - Client disconnect -> server unsubscribes from Redis channels, decrements connection count
    - Server shutdown -> close code 1001, "Server shutting down"
    - Error -> close code 1011, "Internal error"
```

**WebSocket Event Message Format:**

```json
{
  "type": "trade.created",
  "timestamp": "2026-02-11T10:30:00.123Z",
  "data": {
    "trade_id": "abc-123",
    "instrument": "MNQ",
    "direction": "LONG",
    "entry_price": 17500.25
  }
}
```

**Event Type Registry:**

| Event Type | Channel | Payload Fields | Trigger |
|---|---|---|---|
| `trade.created` | `ws:trades:{user_id}` | trade_id, instrument, direction, entry_price, quantity | API creates new trade |
| `trade.updated` | `ws:trades:{user_id}` | trade_id, updated_fields (dict of changed fields) | API updates trade, broker fill received |
| `trade.closed` | `ws:trades:{user_id}` | trade_id, exit_price, pnl_amount, r_multiple, closed_at | Trade exit executed |
| `trendline.alert` | `ws:trendlines:{user_id}` | trendline_id, alert_type, instrument, current_price, trigger_price | Alert condition met |
| `trendline.new` | `ws:trendlines:{user_id}` | trendline_id, instrument, timeframe, direction, strength_score | Scan finds qualifying trendline |
| `scan.complete` | `ws:system:{user_id}` | scan_id, instruments_scanned, trendlines_found, duration_ms | Scheduled scan completes |
| `system.health` | `ws:system:{user_id}` | service_name, status (ok/degraded/error), message | Service status change |

**WebSocket Close Codes:**

| Code | Meaning | When Used |
|---|---|---|
| 1000 | Normal closure | Client or server gracefully closes |
| 1001 | Going away | Server shutdown or heartbeat timeout |
| 1011 | Internal error | Unexpected server error |
| 4000 | Authentication required | No token provided |
| 4001 | Token expired | JWT expired (initial or re-validation) |
| 4003 | Invalid token | JWT signature invalid |
| 4009 | Too many connections | User exceeds 10 connections |

**Client-Side Reconnection (documented for frontend implementation):**
- Reconnect with exponential backoff: 1s, 2s, 4s, 8s, max 30s.
- On reconnect, refresh JWT before connecting.
- After 5 failed reconnection attempts, show user a banner: "Real-time updates unavailable. Data may be stale. Refresh to retry."

### 3.6 Environment Configuration Management

#### 3.6.1 Configuration Architecture (Source: INF-FR-013)

**Description:** All configuration flows through Pydantic Settings with environment variable binding. Zero hardcoded environment-specific values in application code.

**Required Environment Variables:**

| Variable | Type | Required | Default | Description |
|---|---|---|---|---|
| `APP_ENV` | Literal["development", "staging", "production"] | No | `"development"` | Current environment |
| `APP_DEBUG` | bool | No | `False` | Enable debug mode (extra logging, API docs in prod) |
| `APP_VERSION` | str | No | `"0.1.0"` | Application version |
| `LOG_LEVEL` | str | No | `"INFO"` | Minimum log level |
| `DATABASE_URL` | str | **Yes** | -- | PostgreSQL connection string |
| `DATABASE_POOL_SIZE` | int | No | `10` | SQLAlchemy pool size |
| `DATABASE_MAX_OVERFLOW` | int | No | `5` | SQLAlchemy max overflow |
| `UPSTASH_REDIS_URL` | str | **Yes** | -- | Redis connection string (rediss://) |
| `SUPABASE_URL` | str | **Yes** | -- | Supabase project URL |
| `SUPABASE_ANON_KEY` | str | **Yes** | -- | Supabase anonymous key |
| `SUPABASE_SERVICE_ROLE_KEY` | str | **Yes** | -- | Supabase service role key |
| `SUPABASE_JWT_SECRET` | str | **Yes** | -- | JWT signing secret for verification |
| `IBKR_HOST` | str | No | `""` | Interactive Brokers gateway host |
| `IBKR_PORT` | int | No | `4002` | Interactive Brokers gateway port |
| `TRADOVATE_API_KEY` | str | No | `""` | Tradovate API key |
| `TRADOVATE_API_SECRET` | str | No | `""` | Tradovate API secret |
| `SENTRY_DSN` | str | No | `""` | Sentry error tracking DSN |
| `AXIOM_API_TOKEN` | str | No | `""` | Axiom log aggregation token |
| `TELEGRAM_BOT_TOKEN` | str | No | `""` | Telegram bot token for notifications |
| `CORS_ORIGINS` | list[str] | No | `["http://localhost:3000"]` | Allowed CORS origins |
| `RATE_LIMIT_DEFAULT` | int | No | `100` | Default rate limit (requests/minute) |

**Startup Validation Behavior:**

If any **required** variable is missing or empty:
- Log: `"CRITICAL: Required environment variable '{VAR_NAME}' is not set. Application cannot start."`
- Application exits with code 1.
- All missing variables are listed in a single log entry (not one per variable).

If an optional variable has an invalid value (e.g., `APP_ENV=invalid`):
- Pydantic raises a `ValidationError`.
- Log: `"CRITICAL: Invalid configuration: {field_name}: {error_message}"`
- Application exits with code 1.

**`.env.example` Requirement:**

The file `.env.example` MUST contain every variable listed above with placeholder values. Example:

```
APP_ENV=development
DATABASE_URL=postgresql+asyncpg://trendedge:trendedge_dev@localhost:5432/trendedge
UPSTASH_REDIS_URL=redis://localhost:6379/0
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
SUPABASE_JWT_SECRET=your-jwt-secret
```

**What Must NOT Happen:**
- `.env` files must NEVER be committed to git. The `.gitignore` must include `.env`, `.env.local`, `.env.production`, `.env.staging`.
- Secrets must NEVER appear in logs, error messages, API responses, or Sentry breadcrumbs.
- No environment-specific `if APP_ENV == "production"` branches in business logic. Environment differences must be expressed through configuration values only.

#### 3.6.2 Environment Matrix (Source: INF-FR-014)

| Setting | Development | Staging | Production |
|---|---|---|---|
| `APP_ENV` | `development` | `staging` | `production` |
| `APP_DEBUG` | `true` | `false` | `false` |
| `LOG_LEVEL` | `DEBUG` | `INFO` | `INFO` |
| Database | Local PostgreSQL (Docker) | Supabase staging project | Supabase production project |
| Redis | Local Redis (Docker) | Upstash staging instance | Upstash production instance |
| Broker mode | Paper (simulated fills) | Paper (real broker API, paper account) | Live + Paper |
| Sentry | Disabled (`SENTRY_DSN=""`) | Enabled (10% trace sample rate) | Enabled (100% error capture, 10% trace sample) |
| CORS origins | `http://localhost:3000`, `http://127.0.0.1:3000` | `https://staging.trendedge.app` | `https://app.trendedge.app`, `https://trendedge.app` |
| API docs (`/docs`) | Enabled | Enabled | Disabled |
| Rate limiting | Disabled (for development convenience) | Enabled | Enabled |

### 3.7 Health Check Endpoints

#### 3.7.1 Health Check Specification (Source: INF-FR-015)

**Endpoint: `GET /health` (Liveness Probe)**

- **Authentication:** None
- **Rate Limiting:** None (exempt)
- **Purpose:** Confirms the process is running and can handle HTTP requests.
- **Processing:** Return immediately. No dependency checks.

**Success Response (HTTP 200):**

```json
{
  "status": "ok",
  "version": "0.1.0",
  "timestamp": "2026-02-11T10:30:00Z"
}
```

**This endpoint always returns 200 if the process is alive.** It does not check database, Redis, or any other dependency. If this endpoint fails, the process itself is down.

---

**Endpoint: `GET /health/ready` (Readiness Probe)**

- **Authentication:** None
- **Rate Limiting:** None (exempt)
- **Purpose:** Confirms the application and all critical dependencies are operational.
- **Processing:** Check database connectivity (SELECT 1), Redis connectivity (PING), Celery worker availability (inspect ping).

**Success Response (HTTP 200):**

```json
{
  "status": "ok",
  "checks": {
    "database": { "status": "ok", "latency_ms": 12 },
    "redis": { "status": "ok", "latency_ms": 3 },
    "celery": { "status": "ok", "workers": 1 }
  }
}
```

**Degraded Response (HTTP 503):**

```json
{
  "status": "degraded",
  "checks": {
    "database": { "status": "ok", "latency_ms": 12 },
    "redis": { "status": "error", "message": "Connection refused" },
    "celery": { "status": "ok", "workers": 1 }
  }
}
```

**Health Check Behavior:**

| Component | Check Method | Timeout | "ok" Condition | "error" Condition |
|---|---|---|---|---|
| Database | `SELECT 1` via async session | 5 seconds | Query returns within 5s | Query times out or connection fails |
| Redis | `redis_client.ping()` | 5 seconds | PING returns PONG within 5s | Timeout or connection error |
| Celery | `celery_app.control.inspect().ping()` | 10 seconds | At least 1 worker responds | No workers respond within 10s |

**If ANY check returns "error", the overall status is "degraded" and HTTP status is 503.**

**Edge Cases:**
- If database check is slow (>5s) but eventually succeeds, it still counts as "error" (timeout exceeded). The `latency_ms` field is not set; instead `message` contains "Timeout after 5000ms".
- If Celery inspect returns 0 workers but no error, status is "error" with message "No active workers".
- All checks run in parallel (not sequentially) to minimize response time. Total `/health/ready` response time should be <= max(individual check timeouts) = 10 seconds.

---

**Endpoint: `GET /health/detailed` (Diagnostic Endpoint)**

- **Authentication:** API Key required (header `X-API-Key` matching a configured operator key)
- **Rate Limiting:** 10 requests/minute per API key
- **Purpose:** Full system diagnostic for operator troubleshooting.

**Success Response (HTTP 200):**

```json
{
  "status": "ok",
  "version": "0.1.0",
  "environment": "production",
  "uptime_seconds": 86400,
  "checks": {
    "database": {
      "status": "ok",
      "latency_ms": 12,
      "pool_size": 10,
      "pool_checked_out": 3,
      "pool_overflow": 0
    },
    "redis": {
      "status": "ok",
      "latency_ms": 3,
      "connected_clients": 5,
      "used_memory_mb": 2.1
    },
    "celery": {
      "status": "ok",
      "workers": 1,
      "active_tasks": 2,
      "queued_tasks": {
        "high": 0,
        "default": 3,
        "low": 1,
        "notifications": 0
      }
    }
  }
}
```

**Unauthorized Response (HTTP 401):**

```json
{
  "error": {
    "code": "AUTHENTICATION_REQUIRED",
    "message": "Valid API key required for detailed health check.",
    "request_id": "...",
    "timestamp": "..."
  }
}
```

---

## 4. Data Specifications

### 4.1 Infrastructure-Level Data Models

This FSD defines only infrastructure-level data. Application data models (trades, trendlines, users, etc.) are defined in [Cross-reference: FSD-002].

#### 4.1.1 Failed Webhooks Table

| Column | Type | Constraints | Default | Description |
|---|---|---|---|---|
| `id` | UUID | PK | `gen_random_uuid()` | Unique identifier |
| `received_at` | timestamptz | NOT NULL | `now()` | When the webhook was received |
| `source` | varchar(50) | NOT NULL | -- | Origin (e.g., "tradingview") |
| `payload` | jsonb | NOT NULL | -- | Raw webhook payload (secrets redacted) |
| `error_message` | text | NOT NULL | -- | Why processing failed |
| `retry_count` | int | NOT NULL | `0` | Number of retry attempts made |
| `resolved` | boolean | NOT NULL | `false` | Whether the webhook was manually resolved |
| `resolved_at` | timestamptz | NULL | -- | When it was resolved |

#### 4.1.2 Failed Notifications Table

| Column | Type | Constraints | Default | Description |
|---|---|---|---|---|
| `id` | UUID | PK | `gen_random_uuid()` | Unique identifier |
| `user_id` | UUID | FK -> users, NOT NULL | -- | Target user |
| `channel` | varchar(20) | NOT NULL | -- | "telegram", "discord", "email" |
| `message_type` | varchar(50) | NOT NULL | -- | Event type that triggered notification |
| `payload` | jsonb | NOT NULL | -- | Notification content |
| `error_message` | text | NOT NULL | -- | Why delivery failed |
| `retry_count` | int | NOT NULL | `0` | Number of retries attempted |
| `created_at` | timestamptz | NOT NULL | `now()` | When the failure was recorded |

#### 4.1.3 Circuit Breaker State (Redis, not DB)

| Key | Type | TTL | Fields |
|---|---|---|---|
| `circuit:{broker_name}` | JSON string | 15 minutes | `state` (CLOSED/OPEN/HALF_OPEN), `opened_at` (ISO timestamp), `failure_count` (int), `last_failure` (ISO timestamp) |

### 4.2 Data Validation Rules

**Global validation rules applied to all input data:**

| Rule | Constraint | Error Message |
|---|---|---|
| String max length | 10,000 characters (default) | "Field '{field}' exceeds maximum length of {max_length} characters." |
| Numeric bounds | Must be within field-specific min/max | "Field '{field}' must be between {min} and {max}." |
| Enum validation | Must match allowed values | "Invalid value '{value}' for field '{field}'. Allowed values: {allowed}." |
| JSON nesting depth | Maximum 5 levels | "JSON payload exceeds maximum nesting depth of 5." |
| File upload size | Maximum 10 MB | "File size {size_mb}MB exceeds maximum of 10MB." |
| File upload types | PNG, JPG, JPEG, WEBP only | "File type '{type}' not allowed. Accepted types: PNG, JPG, JPEG, WEBP." |
| UUID format | Standard UUID v4 | "Invalid UUID format for field '{field}'." |
| Timestamp format | ISO 8601 with timezone | "Invalid timestamp format for field '{field}'. Expected ISO 8601 (e.g., 2026-02-11T10:30:00Z)." |

---

## 5. API Specifications

### 5.1 Endpoint Overview

All endpoints are prefixed with `/api/v1/` except health checks.

| Method | Path | Auth | Rate Limit | Description |
|---|---|---|---|---|
| GET | `/health` | None | Unlimited | Liveness probe |
| GET | `/health/ready` | None | Unlimited | Readiness probe |
| GET | `/health/detailed` | API Key | 10/min | Full diagnostic |
| POST | `/api/v1/webhooks/tradingview` | API Key + HMAC | 30/min per key | TradingView signal ingestion |
| WS | `/api/v1/ws` | JWT (query param) | 10 connections/user | Real-time event stream |

Additional CRUD endpoints for trades, trendlines, playbooks, analytics, journal, and users are defined in their respective FSDs (FSD-002 through FSD-011). This FSD defines only the infrastructure-level endpoints (health checks, webhooks, WebSocket).

### 5.2 Webhook Endpoint Specification

**Endpoint: `POST /api/v1/webhooks/tradingview`**

**Authentication:** Two-layer authentication:
1. API Key in payload field `api_key` -- matched against stored hash using constant-time comparison.
2. HMAC-SHA256 signature in `X-Signature` header -- `HMAC-SHA256(raw_body, user_webhook_secret)`.

**Request Headers:**

| Header | Required | Description |
|---|---|---|
| `Content-Type` | Yes | Must be `application/json` |
| `X-Signature` | Yes | HMAC-SHA256 hex digest of raw request body |
| `X-Request-ID` | No | Correlation ID (generated if absent) |

**Request Body Example:**

```json
{
  "api_key": "tv_key_abc123",
  "action": "buy",
  "symbol": "NQ1!",
  "price": 17500.25,
  "quantity": 1,
  "timeframe": "1H",
  "strategy": "trendline_break"
}
```

**Processing Logic:**

1. Validate `Content-Type` is `application/json`. If not -> 400: `"Content-Type must be application/json."`
2. Parse JSON body. If malformed -> 400: `"Invalid JSON in request body."`
3. Extract `api_key` from body. If missing -> 401: `"API key is required in webhook payload."`
4. Look up user by API key hash (constant-time comparison). If not found -> 401: `"Invalid API key."` (Do NOT reveal whether the key exists or not -- same message for missing and invalid.)
5. Verify HMAC signature. If `X-Signature` header is missing -> 401: `"HMAC signature required (X-Signature header)."`  If signature does not match -> 401: `"Invalid HMAC signature."`
6. Compute SHA-256 hash of request body. Check Redis for `dedup:webhook:{hash}`. If exists -> 200 (accepted but not re-processed): `{"status": "duplicate", "message": "Webhook already processed within dedup window."}` This is HTTP 200, not an error.
7. Set Redis key `dedup:webhook:{hash}` with 60-second TTL.
8. Validate payload fields via Pydantic. If validation fails -> 400 with field-level errors.
9. Dispatch Celery task `process_webhook` on `high` queue.
10. Return HTTP 202: `{"status": "accepted", "task_id": "celery-task-id-here"}`

**Response Codes:**

| Status | Condition | Body |
|---|---|---|
| 202 | Webhook accepted and queued | `{"status": "accepted", "task_id": "..."}` |
| 200 | Duplicate webhook (dedup hit) | `{"status": "duplicate", "message": "Webhook already processed within dedup window."}` |
| 400 | Invalid JSON, missing fields, validation error | Standard error response |
| 401 | Invalid API key or HMAC signature | Standard error response |
| 429 | Rate limit exceeded | Standard error response with `Retry-After` header |
| 500 | Internal error | Standard error response |

### 5.3 Error Response Format

All API error responses follow the format defined in Section 3.1.4. Every error response includes:

```json
{
  "error": {
    "code": "ERROR_CODE",
    "message": "Human-readable message",
    "details": [],
    "request_id": "uuid",
    "timestamp": "ISO-8601"
  }
}
```

### 5.4 Rate Limiting Headers

All rate-limited endpoints include these response headers:

```
X-RateLimit-Limit: 200
X-RateLimit-Remaining: 150
X-RateLimit-Reset: 1707650400
```

| Header | Type | Description |
|---|---|---|
| `X-RateLimit-Limit` | int | Maximum requests allowed in the current window |
| `X-RateLimit-Remaining` | int | Requests remaining in the current window |
| `X-RateLimit-Reset` | int (Unix timestamp) | When the current window resets |

When rate limited (HTTP 429), also include:

```
Retry-After: 45
```

Where the value is seconds until the window resets.

---

## 6. UI/UX Specifications

### 6.1 Scope

This FSD covers infrastructure only. There are no user-facing UI screens defined here. The frontend infrastructure (Vercel deployment, environment configuration, security headers) is specified in sections 3.6 and 7.4.

For dashboard UI specifications, see [Cross-reference: FSD-007].

### 6.2 Frontend Security Headers

All pages served by Vercel include these security headers (configured in `vercel.json`):

| Header | Value | Purpose |
|---|---|---|
| `X-Frame-Options` | `DENY` | Prevent clickjacking |
| `X-Content-Type-Options` | `nosniff` | Prevent MIME type sniffing |
| `Referrer-Policy` | `strict-origin-when-cross-origin` | Limit referrer information |
| `Permissions-Policy` | `camera=(), microphone=(), geolocation=()` | Disable unnecessary browser APIs |
| `Strict-Transport-Security` | `max-age=31536000; includeSubDomains` | Force HTTPS for 1 year |

### 6.3 Frontend Environment Variables

| Variable | Prefix | Description |
|---|---|---|
| `NEXT_PUBLIC_API_URL` | `NEXT_PUBLIC_` (client-exposed) | Backend API URL |
| `NEXT_PUBLIC_SUPABASE_URL` | `NEXT_PUBLIC_` | Supabase project URL |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | `NEXT_PUBLIC_` | Supabase anonymous key |
| `NEXT_PUBLIC_WS_URL` | `NEXT_PUBLIC_` | WebSocket URL |

**What Must NOT Be Exposed:**
- `SUPABASE_SERVICE_ROLE_KEY` -- server-side only. NEVER prefixed with `NEXT_PUBLIC_`.
- Any broker credentials -- server-side only.
- `SUPABASE_JWT_SECRET` -- server-side only.

---

## 7. Integration Specifications

### 7.1 Supabase Integration

**Provider:** Supabase (PostgreSQL 16, Auth, Storage)
**Purpose:** Primary database, authentication provider, file storage
**Criticality:** Critical -- system non-functional without it

**Services Used:**

| Service | Purpose | Connection Method |
|---|---|---|
| PostgreSQL 16 | Data persistence | TCP via Supavisor (6543) or direct (5432) |
| Supabase Auth | User authentication, JWT issuance | HTTPS API (via Supabase SDK) |
| Supabase Storage | File uploads (chart screenshots, exports) | HTTPS (S3-compatible API) |
| Supavisor | Connection pooling | TCP port 6543 (transaction mode) |

**Error Handling:**

| Error | Cause | Response |
|---|---|---|
| Connection refused (PostgreSQL) | Supabase outage or network issue | Return 503 to client. Log ERROR. Health check returns degraded. |
| Authentication API timeout (>10s) | Supabase Auth service slow | Return 503 to client: "Authentication service temporarily unavailable." Retry once after 2s. |
| Storage upload failure | Supabase Storage outage or quota exceeded | Return 503 to client: "File upload temporarily unavailable." Log ERROR. Queue retry. |
| Invalid JWT from Supabase | Token corrupted or secret mismatch | Return 401 to client: "Authentication failed." Log WARNING with token metadata (NOT the token itself). |

**Fallback Behavior:**
- If Supabase is fully down (all services), the application is non-functional. Display: "Service temporarily unavailable. Please try again in a few minutes." on all endpoints.
- There is no self-hosted fallback. Recovery depends on Supabase's SLA.

### 7.2 Upstash Redis Integration

**Provider:** Upstash (Serverless Redis)
**Purpose:** Caching, Celery broker, pub/sub, rate limiting, distributed locks
**Criticality:** Critical (degraded mode possible)

**Error Handling:**

| Error | Cause | Response |
|---|---|---|
| Connection refused | Upstash outage | Enter degraded mode (see 3.1.2). Log WARNING. |
| Timeout (>5s) | Network latency or Upstash load | Retry once. If still timeout, enter degraded mode for this operation. |
| `READONLY` error | Upstash maintenance | Treat as temporary. Retry after 5s. If persists for >1 minute, enter degraded mode. |
| Memory limit exceeded | Too many keys | Evict cache keys (LRU). Log WARNING. Critical keys (rate limits, locks) are not evicted. |

### 7.3 Sentry Integration

**Provider:** Sentry (Error tracking)
**Purpose:** Capture unhandled exceptions, performance monitoring
**Criticality:** High (monitoring-only, does not affect application functionality)

**Configuration:**

| Setting | Development | Staging | Production |
|---|---|---|---|
| Enabled | No | Yes | Yes |
| Error sample rate | N/A | 100% | 100% |
| Trace sample rate | N/A | 10% | 10% |
| Environment tag | `development` | `staging` | `production` |

**Context Attached to Every Event:**
- `request_id` (from middleware)
- `user_id` (from JWT, if authenticated)
- `environment` (dev/staging/prod)
- `app_version`
- HTTP method, path, status code
- Request headers (excluding `Authorization`, `Cookie`)

**What Must NOT Be Sent to Sentry:**
- JWT tokens
- API keys
- Passwords
- Broker credentials
- Request bodies containing user financial data

### 7.4 Vercel Integration

**Provider:** Vercel (Frontend hosting)
**Purpose:** Host Next.js application with edge network, automatic SSL, preview deployments

**Deployment Flow:**

| Trigger | Action | Result |
|---|---|---|
| Push to any branch | Vercel creates Preview Deployment | Unique URL for testing |
| Merge to `main` | Vercel promotes to Production | `app.trendedge.app` updated |
| Environment variables change in Vercel dashboard | Requires redeployment | Values available on next deploy |

**Vercel Configuration (`vercel.json`):**

| Setting | Value |
|---|---|
| Framework | `nextjs` |
| Build command | `pnpm build` |
| Install command | `pnpm install --frozen-lockfile` |
| Output directory | `.next` |
| Region | `iad1` (US East -- closest to Railway and Supabase) |

### 7.5 Railway Integration

**Provider:** Railway (Backend hosting)
**Purpose:** Host FastAPI API, Celery workers, Celery Beat

**Services:**

| Service | Start Command | Resources (MVP) | Health Check |
|---|---|---|---|
| `trendedge-api` | `gunicorn app.main:app -k uvicorn.workers.UvicornWorker -b 0.0.0.0:$PORT --workers 2 --timeout 120` | 512MB RAM, 0.5 vCPU | GET /health |
| `trendedge-worker` | `celery -A app.tasks.celery_app worker --loglevel=info -Q high,default,low,notifications -c 4` | 512MB RAM, 0.5 vCPU | Celery inspect ping |
| `trendedge-beat` | `celery -A app.tasks.celery_app beat --loglevel=info` | 256MB RAM, 0.25 vCPU | Process alive check |

**Railway Configuration (`railway.toml`):**

```toml
[build]
builder = "nixpacks"

[deploy]
startCommand = "gunicorn app.main:app -k uvicorn.workers.UvicornWorker -b 0.0.0.0:$PORT --workers 2 --timeout 120"
healthcheckPath = "/health"
healthcheckTimeout = 10
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 5
```

**Restart Behavior:**
- `ON_FAILURE` policy: Railway restarts the service if it exits with a non-zero code.
- Maximum 5 restart attempts. After 5 failures, service stays down.
- On 5th failure: Telegram alert: `"CRITICAL: Service {service_name} failed to restart after 5 attempts. Manual intervention required."`

### 7.6 Axiom Integration

**Provider:** Axiom (Log aggregation)
**Purpose:** Centralized log search and dashboards
**Criticality:** Medium

**Setup:**
- Railway logs forwarded to Axiom via Railway's native log drain integration.
- Vercel logs forwarded via Axiom's Vercel integration.
- Retention: 30 days (Axiom free tier).
- Ingestion limit: 500 MB/month (free tier).

### 7.7 Uptime Robot Integration

**Provider:** Uptime Robot (Health monitoring)
**Purpose:** External health check monitoring and alerting
**Criticality:** Medium

**Monitors:**

| Monitor | URL | Check Interval | Alert Condition | Alert Channel |
|---|---|---|---|---|
| API Readiness | `https://api.trendedge.app/health/ready` | 5 minutes | Non-200 response for 2 consecutive checks | Telegram + Email |
| API Liveness | `https://api.trendedge.app/health` | 5 minutes | Non-200 response for 1 check | Telegram + Email |
| Frontend | `https://app.trendedge.app` | 5 minutes | Non-200 response for 2 consecutive checks | Email |
| Staging API | `https://staging-api.trendedge.app/health` | 15 minutes | Non-200 for 2 checks | Email only |

### 7.8 Internal Module Dependencies

```
api/v1/router.py
    -> health.py (no auth, no dependencies)
    -> webhooks.py -> core/security.py (HMAC verification)
                   -> services/trade_service.py -> db/repositories/
                   -> tasks/celery_app.py (dispatch async task)
    -> ws.py -> core/security.py (JWT validation)
            -> Redis pub/sub (subscribe to channels)
    -> trades.py -> services/trade_service.py
    -> trendlines.py -> services/trendline_service.py
    (etc.)

core/middleware.py
    -> core/security.py (rate limit checks)
    -> Redis client (rate limit counters)
    -> core/logging.py (structured log output)

tasks/celery_app.py
    -> Redis (broker + result backend)
    -> db/session.py (database access from workers)
    -> adapters/ (broker API calls)
    -> services/ (business logic)
```

---

## 8. Security Specifications

### 8.1 Authentication Architecture (Source: INF-SEC-001)

**Authentication Flow:**

```
[1] User logs in via Supabase Auth (email/password, OAuth, or magic link)
    |
    v
[2] Supabase issues JWT (access token + refresh token)
    |   Access token TTL: 1 hour
    |   Refresh token TTL: 7 days
    |   Algorithm: HS256
    |
    v
[3] Client stores tokens and sends access token on every request:
    |   Header: Authorization: Bearer <access_token>
    |
    v
[4] FastAPI middleware validates JWT on every protected request:
    |   Step 1: Extract token from Authorization header
    |   Step 2: Verify signature using SUPABASE_JWT_SECRET
    |   Step 3: Check expiration (exp claim)
    |   Step 4: Extract user_id, email, role from claims
    |   Step 5: Attach to request.state.user
    |
    v
[5] Route handler accesses request.state.user for authorization
```

**JWT Validation Error Responses:**

| Condition | HTTP Status | Error Code | Message |
|---|---|---|---|
| No `Authorization` header | 401 | `AUTHENTICATION_REQUIRED` | "Authentication required. Please provide a valid access token." |
| Malformed header (not "Bearer <token>") | 401 | `AUTHENTICATION_REQUIRED` | "Invalid Authorization header format. Expected: Bearer <token>" |
| Invalid signature | 401 | `AUTHENTICATION_REQUIRED` | "Invalid access token." |
| Expired token | 401 | `AUTHENTICATION_REQUIRED` | "Access token expired. Please refresh your token." |
| Token missing required claims | 401 | `AUTHENTICATION_REQUIRED` | "Invalid access token." |

**What Must NOT Happen:**
- Error messages must NOT reveal whether a token is expired vs. invalid signature vs. malformed. In production, all JWT errors return the same generic message: `"Invalid access token."` The specific messages above are for development mode only (`APP_DEBUG=true`).
- The JWT secret must NEVER be logged or included in error responses.
- JWT validation must use constant-time comparison for signature verification.

### 8.2 Webhook Authentication (Source: INF-SEC-002)

**Two-Layer Authentication:**

1. **API Key:** Each user generates a unique webhook API key via the dashboard. The key is displayed once and stored as a bcrypt hash in the database. The raw key is included in the webhook payload's `api_key` field.

2. **HMAC Signature:** Each user has a webhook secret. The sender computes `HMAC-SHA256(raw_request_body, webhook_secret)` and sends it in the `X-Signature` header.

**Verification Steps:**
1. Extract `api_key` from request body.
2. Look up user by API key hash using constant-time comparison (`hmac.compare_digest`).
3. Extract `X-Signature` header.
4. Compute `HMAC-SHA256(raw_body, user.webhook_secret)`.
5. Compare computed signature with header value using constant-time comparison.
6. Both must pass. If either fails, return 401.

### 8.3 Rate Limiting (Source: INF-SEC-004)

**Rate Limit Configuration:**

| Endpoint Category | Limit | Window | Identifier | On Exceed |
|---|---|---|---|---|
| Authentication (`/auth/*`) | 10 requests | 1 minute | IP address | HTTP 429 + `Retry-After` |
| Webhook ingestion (`/webhooks/*`) | 30 requests | 1 minute | API Key | HTTP 429 + `Retry-After` + log alert |
| Read endpoints (`GET /api/v1/*`) | 200 requests | 1 minute | User ID | HTTP 429 + `Retry-After` |
| Write endpoints (`POST/PUT/DELETE /api/v1/*`) | 60 requests | 1 minute | User ID | HTTP 429 + `Retry-After` |
| Analytics (heavy queries) | 20 requests | 1 minute | User ID | HTTP 429 + `Retry-After` |
| WebSocket connections | 10 connections | Per user (concurrent) | User ID | Connection rejected (close code 4009) |
| Health checks (`/health*`) | Unlimited | N/A | N/A | Never rate limited |
| Detailed health (`/health/detailed`) | 10 requests | 1 minute | API Key | HTTP 429 |

**Implementation:** Redis sliding window counter.

**Key pattern:** `ratelimit:{category}:{identifier}`
**Algorithm:** For each request:
1. Compute key based on category and identifier.
2. `INCR` the key.
3. If this is the first increment (`INCR` returns 1), set TTL to the window duration.
4. If counter > limit, reject with 429.
5. Set response headers `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`.

**Rate Limit Exceeded Response (HTTP 429):**

```json
{
  "error": {
    "code": "RATE_LIMITED",
    "message": "Rate limit exceeded. Try again in 45 seconds.",
    "request_id": "...",
    "timestamp": "..."
  }
}
```

Headers: `Retry-After: 45`

### 8.4 Input Validation (Source: INF-SEC-005)

**Validation Layers:**

| Layer | Tool | What It Validates |
|---|---|---|
| HTTP request | Pydantic v2 models | Types, field constraints, custom validators |
| Database query | SQLAlchemy ORM (parameterized queries) | SQL injection prevention |
| File upload | Content-type + size check | Malicious file prevention |
| WebSocket message | Pydantic models | Message structure and types |
| Webhook payload | Pydantic + HMAC | Structure, types, and authenticity |

**SQL Injection Prevention (Source: INF-SEC-006):**
- All queries go through SQLAlchemy ORM. No raw SQL in application code.
- Any exception requiring raw SQL MUST use `text()` with bound parameters: `session.execute(text("SELECT * FROM users WHERE id = :id"), {"id": user_id})`.
- NEVER use f-strings or string concatenation for SQL.
- Connection encryption: `sslmode=require` on all database connections.

### 8.5 Transport Security (Source: INF-SEC-007)

| Requirement | Implementation |
|---|---|
| All API traffic over HTTPS | Railway automatic SSL. HSTS header. |
| All frontend over HTTPS | Vercel automatic SSL. HTTP -> HTTPS redirect. |
| Database connections encrypted | `sslmode=require` in connection string. |
| Redis connections encrypted | `rediss://` (TLS). Plaintext `redis://` rejected in staging/production. |
| WebSocket over WSS | Enforced by browser when served over HTTPS. |
| Minimum TLS version | TLS 1.2 (enforced by hosting platforms). |

**Security Headers (all responses):**

```
Strict-Transport-Security: max-age=31536000; includeSubDomains
X-Content-Type-Options: nosniff
X-Frame-Options: DENY
X-XSS-Protection: 0
Referrer-Policy: strict-origin-when-cross-origin
Permissions-Policy: camera=(), microphone=(), geolocation=()
```

### 8.6 Secrets Management (Source: INF-DEVOPS-008, INF-SEC-008)

**Storage Locations:**

| Secret | Storage | Rotation |
|---|---|---|
| Database credentials | Supabase dashboard + Railway env vars | On compromise |
| Redis URL | Upstash dashboard + Railway env vars | On compromise |
| Supabase service role key | Supabase dashboard + Railway env vars | On compromise |
| Supabase JWT secret | Supabase dashboard + Railway env vars | On compromise (invalidates all sessions) |
| Broker API keys | Railway env vars (encrypted) | Every 90 days |
| Sentry DSN | Railway env vars | On compromise |
| Telegram bot token | Railway env vars | On compromise |
| GitHub Actions secrets | GitHub repository settings | On compromise |
| Vercel env vars | Vercel dashboard | On compromise |
| User webhook API keys | Database (bcrypt hashed) | User-initiated or every 180 days |

**Broker Credential Encryption at Rest:**
- Broker credentials stored in the database MUST be encrypted using AES-256-GCM (application-level encryption).
- Encryption key stored as environment variable `BROKER_ENCRYPTION_KEY`.
- The encryption key itself is NOT stored in the database.

**Rules:**
1. `.env` files in `.gitignore`. Only `.env.example` in repository.
2. No secret in logs, error messages, or API responses.
3. All CI/CD secrets use GitHub Actions encrypted secrets.
4. Secrets must NEVER be committed to git, even in private repositories.

### 8.7 Audit Logging

**Security-relevant events that must be logged:**

| Event | Log Level | Fields Logged |
|---|---|---|
| Successful login | INFO | user_id, ip, user_agent, method (password/oauth/magic_link) |
| Failed login | WARNING | ip, user_agent, reason (invalid_password/user_not_found/account_locked) |
| Rate limit exceeded | WARNING | identifier, category, limit, current_count |
| JWT validation failure | WARNING | request_id, ip, reason (expired/invalid/missing) |
| Webhook authentication failure | WARNING | ip, api_key_prefix (first 4 chars only), reason |
| Circuit breaker state change | WARNING/INFO | broker_name, from_state, to_state, failure_count |
| Secret rotation | INFO | secret_type, rotated_by |
| Admin action (future) | INFO | admin_user_id, action, target_resource |

---

## 9. Performance Specifications

### 9.1 Response Time Requirements

| ID | Operation | Target (p95) | Measurement Point |
|---|---|---|---|
| INF-NFR-001 | Webhook-to-paper-trade latency | < 10 seconds | POST received to simulated fill logged in DB |
| INF-NFR-002 | CRUD API responses | < 200ms | Application layer (excluding network) |
| INF-NFR-003 | Analytics query responses | < 2 seconds | With Redis caching for repeated queries |
| INF-NFR-004 | WebSocket message delivery | < 500ms | From event publish (Redis) to client receipt |
| INF-NFR-005 | Dashboard initial load (LCP) | < 2.5 seconds | Lighthouse on simulated 4G connection |
| INF-NFR-006 | Trendline scan (single instrument) | < 30 seconds | Data fetch + detection + scoring |
| INF-NFR-007 | Database query time | < 100ms | Simple indexed queries via connection pool |

### 9.2 Availability Requirements

| ID | Component | Target | Measurement Window |
|---|---|---|---|
| INF-NFR-008 | Execution pipeline | 99.5% (~43 min downtime/month) | Market hours: Sun 5PM - Fri 4PM CT |
| INF-NFR-009 | Dashboard | 99.0% (~7.3 hrs downtime/month) | 24/7 |
| INF-NFR-010 | Webhook endpoint | 99.5% | 24/7 (TradingView retries once) |
| INF-NFR-011 | Planned maintenance | Weekends only | Sat 6PM - Sun 3PM CT |

### 9.3 Scalability Targets

| Metric | Phase 1 (MVP) | Phase 2 (Growth) | Phase 3 (Scale) |
|---|---|---|---|
| Concurrent users | 1-5 | 50-100 | 500-1,000+ |
| API requests/minute | 100 | 1,000 | 10,000 |
| WebSocket connections | 5 | 100 | 1,000 |
| Celery tasks/minute | 20 | 200 | 2,000 |
| Database size | < 1 GB | < 10 GB | < 100 GB |
| Worker instances | 1 | 2-3 | 5-10 |

**Scaling Triggers (automated alerts, manual action):**

| Condition | Duration | Action |
|---|---|---|
| Task queue backlog > 100 tasks | > 5 minutes | Add worker instance on Railway |
| Database connections > 70% of pool | Sustained 10 minutes | Upgrade pool size or add read replicas |
| API p95 latency > 500ms | > 10 minutes | Add API instance on Railway |
| Redis commands > 8,000/day | 3 consecutive days | Evaluate Upstash paid plan |

### 9.4 Caching Strategy

| Data | Cache Location | TTL | Invalidation |
|---|---|---|---|
| Analytics aggregations | Redis | 300 seconds | On trade create/update/delete |
| User settings | Redis | 600 seconds | On settings update |
| Trendline scan results | Redis | 60 seconds | On new scan completion |
| Health check (internal) | In-memory | 10 seconds | Automatic expiry |
| Static assets | Vercel CDN | Browser: 1 year (immutable hashes) | On redeploy |

### 9.5 Resource Limits

| Resource | Limit | Enforcement |
|---|---|---|
| API request body size | 1 MB | FastAPI/Gunicorn configuration |
| File upload size | 10 MB | Application-level check before processing |
| WebSocket message size | 64 KB | FastAPI WebSocket configuration |
| Database query timeout | 30 seconds | `command_timeout` on connection |
| Celery task timeout | 600 seconds (10 min) | `visibility_timeout` in broker transport options |
| Worker memory per child | 512 MB | `worker_max_memory_per_child` Celery setting |

---

## 10. Testing Specifications

### 10.1 Unit Test Specifications

**Framework:** `pytest` with `pytest-asyncio`
**Coverage Target:** 80% line coverage for `app/` (enforced in CI -- build fails below 80%)
**Exclusions:** `alembic/`, `tests/`, `__init__.py`

**Required Unit Test Scenarios:**

| Area | Test Scenario | Expected Input | Expected Output |
|---|---|---|---|
| Error handler | Pydantic ValidationError produces 400 | Invalid request body (missing required field) | HTTP 400, error code `VALIDATION_ERROR`, details array with field name |
| Error handler | Unhandled exception produces 500 | Raise `RuntimeError` in handler | HTTP 500, error code `INTERNAL_ERROR`, no stack trace in response |
| Error handler | 5xx responses do not leak internals | Raise `asyncpg.PostgresError` | Response body does NOT contain table names, SQL, or file paths |
| JWT validation | Valid JWT accepted | Well-formed JWT signed with correct secret | `request.state.user` populated with user_id |
| JWT validation | Expired JWT rejected | JWT with `exp` in the past | HTTP 401, error code `AUTHENTICATION_REQUIRED` |
| JWT validation | Invalid signature rejected | JWT signed with wrong secret | HTTP 401, error code `AUTHENTICATION_REQUIRED` |
| JWT validation | Missing Authorization header rejected | Request with no Authorization header | HTTP 401, error code `AUTHENTICATION_REQUIRED` |
| Rate limiting | Under limit passes | 5 requests within 200/min limit | All 5 return 200 |
| Rate limiting | Over limit blocked | 201 requests within 200/min limit | Request 201 returns 429 with `Retry-After` header |
| Rate limiting | Different users have independent limits | User A at limit, User B makes request | User B's request passes (200) |
| Webhook auth | Valid API key + HMAC accepted | Correct key and signature | HTTP 202 |
| Webhook auth | Invalid API key rejected | Nonexistent key | HTTP 401 |
| Webhook auth | Invalid HMAC rejected | Correct key, wrong signature | HTTP 401 |
| Webhook dedup | Duplicate payload returns 200 | Same payload sent twice within 60s | Second request returns 200 with `"status": "duplicate"` |
| Config validation | Missing required env var | `DATABASE_URL` not set | Application raises SystemExit(1) |
| Config validation | Invalid env var value | `APP_ENV=invalid` | Application raises SystemExit(1) |
| Health liveness | Returns 200 when alive | GET /health | HTTP 200, body contains `"status": "ok"` and `"version"` |
| Health readiness | Returns 503 when DB down | Mock DB to fail | HTTP 503, body contains `"status": "degraded"` |
| Middleware | Request ID assigned | Request without X-Request-ID | Response has X-Request-ID header with valid UUID |
| Middleware | Existing Request ID preserved | Request with X-Request-ID: "abc-123" | Response has same X-Request-ID: "abc-123" |
| Circuit breaker | Opens after 3 failures | 3 consecutive broker failures in 5 min | Circuit state is OPEN, next task immediately rejected |
| Circuit breaker | Closes after recovery | HALF_OPEN state, task succeeds | Circuit state is CLOSED |

### 10.2 Integration Test Specifications

**Dependencies:** PostgreSQL 16 (Docker), Redis 7 (Docker)

**Test Database Strategy:**
1. CI spins up PostgreSQL 16 and Redis 7 as GitHub Actions services.
2. Alembic runs all migrations against test database.
3. Each test uses a database transaction that is rolled back after completion.
4. No shared state between tests.

**Required Integration Test Scenarios:**

| Test | Setup | Action | Verification |
|---|---|---|---|
| DB connection | Start with Supavisor-compatible settings | Create and read a record | Record persists and is retrievable |
| DB pool exhaustion | Set pool_size=1, max_overflow=0 | Make 2 concurrent DB requests | Second request waits then completes (or times out) |
| Redis cache | Empty Redis | Set cache key, get cache key | Value matches. TTL is set. |
| Redis unavailable | Stop Redis | Make API request | Request succeeds (degraded mode). No 500 error. |
| Celery task dispatch | Worker in eager mode | Dispatch task and wait | Task completes, result stored in Redis |
| Full webhook flow | DB + Redis + eager Celery | POST to /webhooks/tradingview with valid auth | 202 returned, task executed, data in DB |
| WebSocket connect | API + Redis | Connect with valid JWT | Connection accepted, receive events |
| WebSocket auth failure | API | Connect with invalid JWT | Connection rejected with close code 4003 |
| Migration roundtrip | Empty DB | `upgrade head`, `downgrade base`, `upgrade head` | All succeed, no errors |

### 10.3 Load Test Specifications

| Scenario | Tool | Target | When |
|---|---|---|---|
| API throughput | `locust` or `k6` | 100 requests/second sustained for 5 minutes, p95 < 200ms | Phase 1 |
| Webhook burst | `k6` | 30 webhooks in 10 seconds with 0 drops | Phase 1 |
| WebSocket connections | `k6` | 100 concurrent connections | Phase 2 |
| Database query perf | `pgbench` + custom | p95 < 100ms under load | Phase 2 |
| Full pipeline | Custom script | Webhook to journal entry in < 10s under load | Phase 2 |

Load tests run against staging only, never production. Results stored in `tests/load/results/`.

### 10.4 CI Pipeline Test Stages

```
Stage 1: Static Analysis (target: < 60 seconds)
    - ruff check .
    - ruff format --check .
    - mypy app/ --ignore-missing-imports
    - ESLint (frontend)
    - TypeScript check (frontend)

Stage 2: Unit Tests (target: < 120 seconds)
    - pytest tests/unit/ -v --cov=app --cov-report=xml
    - vitest (frontend)

Stage 3: Integration Tests (target: < 300 seconds)
    - pytest tests/integration/ -v (with PostgreSQL + Redis services)

Stage 4: Build Verification (target: < 180 seconds)
    - docker build . (backend)
    - pnpm build (frontend)

Total CI target: < 10 minutes
```

**CI Failure Behavior:**
- Stage 1 failure: PR blocked. Developer must fix lint/type errors.
- Stage 2 failure: PR blocked. Developer must fix failing tests.
- Stage 3 failure: PR blocked. Developer must fix integration issues.
- Stage 4 failure: PR blocked. Developer must fix build issues.
- Coverage below 80%: PR blocked. Coverage report shows uncovered lines.

---

## 11. Migration & Deployment

### 11.1 CI/CD Pipeline Architecture (Source: INF-DEVOPS-001 through INF-DEVOPS-003)

**Backend Deployment Flow:**

```
Push to feature branch
    |
    v
[CI: lint + type check + unit tests + integration tests + build]
    |   (Triggered by: push to any branch, PR to main)
    |   (Duration target: < 10 minutes)
    |
    v
Pull Request to main
    |   (Requires: 1 approval, all CI checks green)
    |
    v
Merge to main
    |
    +---> [CD: Deploy to Railway staging]
    |       |
    |       v
    |     [Run: alembic upgrade head (staging DB)]
    |       |
    |       v
    |     [Smoke test: curl https://staging-api.trendedge.app/health/ready]
    |       |   Success: continue
    |       |   Failure: alert Telegram, halt pipeline
    |       |
    |       v
    |     [Manual approval gate] (Phase 2+ only)
    |       |
    |       v
    |     [CD: Deploy to Railway production]
    |       |
    |       v
    |     [Run: alembic upgrade head (production DB)]
    |       |
    |       v
    |     [Health check: curl https://api.trendedge.app/health/ready]
    |       |   Success: send Telegram "Deployed {sha} to production"
    |       |   Failure: alert Telegram CRITICAL, manual rollback needed
    |
    +---> [CD: Vercel auto-deploys frontend]
            Preview on PR -> Production on merge to main
```

**Frontend Deployment Flow:**

1. Push to any branch: Vercel creates a Preview Deployment with a unique URL.
2. PR merge to `main`: Vercel promotes to Production.
3. No manual steps required. Environment variables managed in Vercel dashboard.

### 11.2 Database Migration Steps

**For every deployment that includes migration files:**

1. CI validates migration can upgrade AND downgrade on ephemeral test DB.
2. CD applies `alembic upgrade head` to staging DB.
3. Staging health check confirms database is operational.
4. (Phase 2+) Manual approval for production.
5. CD applies `alembic upgrade head` to production DB.
6. Production health check confirms database is operational.

**Rollback Procedure:**

If a migration causes issues in production:
1. Do NOT use `alembic downgrade` in production without careful review -- downgrade may cause data loss.
2. Preferred: deploy a new migration that reverses the change (forward-fix).
3. If immediate rollback needed: `alembic downgrade -1` after confirming the downgrade function is safe and data-preserving.
4. Redeploy the previous application version (git revert and push, or Railway rollback to previous deployment).

### 11.3 Feature Flag Requirements

Phase 1 does not require a feature flag system. All features are deployed to all users.

Phase 2+ should implement feature flags via environment variables or a simple database table for gradual rollouts. This is deferred.

### 11.4 Deployment Dependencies

| Component | Depends On | Must Deploy First |
|---|---|---|
| Database migrations | Database connectivity | Database must be accessible |
| Backend API | Database + Redis | Database and Redis must be running |
| Celery Worker | Database + Redis | Database and Redis must be running |
| Celery Beat | Redis | Redis must be running |
| Frontend | Backend API (runtime) | Backend should be deployed first or simultaneously |

**Deployment Order:** Database migrations -> Backend API -> Celery Worker -> Celery Beat -> Frontend (Vercel deploys independently)

### 11.5 Rollback Procedures

| Component | Rollback Method | Time Estimate |
|---|---|---|
| Backend API | Railway: redeploy previous deployment from dashboard | < 5 minutes |
| Celery Worker | Railway: redeploy previous deployment | < 5 minutes |
| Frontend | Vercel: promote previous deployment from dashboard | < 2 minutes |
| Database migration | Forward-fix preferred. Emergency: `alembic downgrade -1` | 5-30 minutes |

### 11.6 Disaster Recovery

| Scenario | Detection | Recovery | RTO | RPO |
|---|---|---|---|---|
| API crash | Uptime Robot (2 min) | Railway auto-restart (ON_FAILURE). If persistent: redeploy last known-good. | 5 min | 0 (stateless) |
| Database corruption | App errors + Sentry | Restore from Supabase backup or pg_dump. | 30-60 min | 24h (Free) / min (Pro) |
| Redis failure | Health check 503 | Upstash auto-recovery. API degrades gracefully. | 5 min | 0 (rebuildable) |
| Worker crash | Tasks stop, queue grows | Railway auto-restart. Unfinished tasks redelivered. | 5 min | 0 (redelivered) |
| Supabase outage | Health check 503 | Wait for recovery. No fallback. Notify users. | Depends on Supabase | Depends on Supabase |
| Railway outage | Uptime Robot (2 min) | If > 30 min: emergency deploy to Render/Fly.io. | 30-60 min | 0 (stateless) |
| Secret compromise | Audit log anomaly | Rotate all affected secrets. Invalidate sessions. Audit logs. | 1-4 hours | N/A |
| Full data loss | Cannot access any data | Restore from pg_dump (R2, 30-day retention). | 2-4 hours | 24 hours |

**Recovery Priority Order:**
1. Database (data is irreplaceable)
2. API server (user-facing)
3. Celery workers (async processing)
4. Celery Beat (periodic tasks)
5. Frontend (Vercel has independent uptime)

---

## 12. Open Questions & Assumptions

### 12.1 Assumptions

| # | Assumption | Impact if Wrong |
|---|---|---|
| 1 | Supabase free tier provides sufficient database capacity (500MB) for Phase 1 with < 5 users. | Need to upgrade to Pro ($25/mo) earlier than planned. |
| 2 | Upstash free tier (10K commands/day) is sufficient for MVP. | Need paid plan ($0.20/100K commands). Adjust cost projections. |
| 3 | A single Railway worker instance with concurrency 4 handles all task queues for Phase 1. | Need to split into dedicated workers per queue earlier. |
| 4 | TradingView webhook IPs are not stable enough for IP allowlisting as primary auth. | If they stabilize, could add as supplementary security layer. |
| 5 | HS256 JWT algorithm (Supabase default) is acceptable. RS256 not required. | If asymmetric signing needed (e.g., microservices verifying without shared secret), migration required. |
| 6 | Railway's ON_FAILURE restart policy with 5 retries is sufficient for auto-recovery. | May need external health check + restart mechanism. |
| 7 | 30-day log retention (Axiom free tier) is sufficient for debugging and compliance. | Need Axiom paid plan or alternative long-term storage. |
| 8 | The Supabase connection limit on free tier (60 direct connections) is not hit with pool_size=10 + overflow=5 for API and pool_size=5 + overflow=3 for worker. | Need Supabase Pro or reduce pool sizes. |

### 12.2 Items Requiring Clarification

| # | Question | Impact | Default if Unresolved |
|---|---|---|---|
| 1 | Should the `/health/detailed` endpoint require a dedicated operator API key or use the Supabase service role key? | Security of diagnostic data | Use a dedicated `HEALTH_CHECK_API_KEY` env var. |
| 2 | Should WebSocket connections be terminated on Railway or use a dedicated WebSocket service? | Scalability of real-time features at Phase 3 | Start on Railway; evaluate dedicated service at Phase 3. |
| 3 | What is the exact Content-Security-Policy directive? The PRD mentions it but does not specify the full value. | Frontend security posture | Use: `default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; connect-src 'self' https://*.supabase.co wss://*.trendedge.app;` |
| 4 | Should database backups (pg_dump) be stored on Cloudflare R2 or Supabase Storage? | Cost and reliability of backup storage | Use Supabase Storage for simplicity (already integrated). Migrate to R2 if storage limits hit. |
| 5 | Is the `BROKER_ENCRYPTION_KEY` for AES-256-GCM encryption of broker credentials a new requirement not in the PRD, or is `pgcrypto` preferred? | Implementation approach for credential encryption | Use application-level AES-256-GCM (more portable, does not depend on PostgreSQL extension). |

### 12.3 Deferred Decisions

| # | Decision | Deferred To | Reason |
|---|---|---|---|
| 1 | Row-Level Security (RLS) policy design | Phase 3 (multi-tenant) | Not needed for single-user MVP |
| 2 | Read replica configuration | Phase 2-3 | Not needed until analytics query load justifies it |
| 3 | Feature flag system | Phase 2 | Simple on/off via env vars is sufficient for Phase 1 |
| 4 | CDN configuration for API responses | Phase 3 | Traffic too low to justify in Phase 1 |
| 5 | Horizontal API scaling (multiple Railway instances) | Phase 2-3 | Single instance sufficient for MVP load |

---

## 13. Appendices

### 13.1 Glossary

| Term | Definition |
|---|---|
| **Supavisor** | Supabase's built-in connection pooler (replaces PgBouncer). Runs on port 6543 in transaction mode. Required for connection pooling with Supabase. |
| **RLS** | Row-Level Security. PostgreSQL feature that restricts row access per-user. Used by Supabase for multi-tenant data isolation. |
| **Circuit Breaker** | A pattern that stops calling a failing external service after N consecutive failures, waiting a cooldown period before retrying. Prevents cascading failures. |
| **Nixpacks** | Railway's build system that auto-detects the programming language and creates optimized container images without a Dockerfile. |
| **PITR** | Point-in-Time Recovery. Database restore to any moment using Write-Ahead Log (WAL) archives. Available on Supabase Pro tier. |
| **Idempotency** | The property that performing an operation multiple times produces the same result as performing it once. Critical for webhook processing to prevent duplicate trades. |
| **MAE/MFE** | Maximum Adverse/Favorable Excursion. Trade analytics metrics measuring the worst and best price movement during a trade's lifetime. |
| **Degraded Mode** | Application state where a non-critical dependency (Redis) is unavailable. Core functionality continues with reduced features (no caching, no real-time updates, no rate limiting). |
| **Visibility Timeout** | Celery broker setting controlling how long a task is invisible to other workers after being claimed. If the worker crashes before completing the task, it becomes visible again for redelivery after this timeout. |
| **Supavisor Transaction Mode** | Connection pooling mode where each database transaction gets a connection from the pool, which is returned after the transaction completes. Does not support prepared statements (`statement_cache_size=0` required). |

### 13.2 Related FSDs

| FSD | Title | Relationship to This FSD |
|---|---|---|
| FSD-002 | Data Models & Database Schema | Defines schemas on the database infrastructure set up here |
| FSD-003 | Trendline Detection Engine | Runs on the Celery worker infrastructure defined here |
| FSD-004 | Broker Adapters | Uses environment config, secrets, and circuit breaker from here |
| FSD-005 | Trade Execution Pipeline | Uses webhook endpoint, WebSocket, and Redis pub/sub from here |
| FSD-006 | Journaling & Playbooks | Uses file storage and database transactions from here |
| FSD-007 | Dashboard UI | Deployed on Vercel infrastructure defined here; uses CORS and WebSocket |
| FSD-008 | Notifications | Uses Celery notification queue and Telegram config from here |
| FSD-009 | AI/ML Features | Uses Celery low-priority queue and API credentials from here |
| FSD-010 | Billing & Subscriptions | Uses Supabase Auth and environment config from here |
| FSD-011 | Analytics Engine | Uses Redis caching and database optimization from here |

### 13.3 Cost Breakdown by Phase

| Phase | Monthly Cost | Breakdown |
|---|---|---|
| Phase 1 (MVP, 0-5 users) | $30-90 | Railway $15-30, Supabase $0, Upstash $0, Vercel $0, Sentry $0, Axiom $0, Domain ~$1, Claude API $5-20 |
| Phase 2 (Growth, 5-100 users) | $50-150 | Railway $25-50, Supabase Pro $25, Upstash $0-10, Vercel $0, remaining $0-20 |
| Phase 3 (Scale, 100-1K users) | $250-500 | Railway $100-200, Supabase Pro $25-50, Upstash $10-25, Vercel $0-20, monitoring $0-25 |

### 13.4 Decision Log

| Decision | Options Considered | Chosen | Rationale |
|---|---|---|---|
| Primary database | Supabase PostgreSQL, Railway PostgreSQL, PlanetScale | Supabase | Built-in Auth, Storage, RLS, free tier, dashboard |
| Backend hosting | Railway, Render, Fly.io, AWS ECS | Railway | Best DX, persistent processes, simple scaling, built-in metrics |
| Frontend hosting | Vercel, Netlify, Cloudflare Pages | Vercel | Native Next.js support, edge functions, preview deployments |
| Task queue | Celery + Redis, Dramatiq, ARQ, Huey | Celery + Redis | Most mature ecosystem, Upstash doubles as cache + broker |
| Redis provider | Upstash, Railway Redis, ElastiCache | Upstash | Serverless (no idle cost), TLS built-in, free tier |
| Migration tool | Alembic, Supabase Migrations CLI | Alembic | Native SQLAlchemy integration, auto-generation |
| Auth provider | Supabase Auth, Clerk, Auth0, NextAuth | Supabase Auth | Already using Supabase, generous free tier, JWT-based |
| Monitoring stack | Sentry + Axiom + Uptime Robot vs. Datadog vs. New Relic | Sentry + Axiom + Uptime Robot | All free tier for MVP. Combined cost: $0 |
| Logging format | Structured JSON vs. plaintext | Structured JSON | Machine-parseable, Axiom-native, enables log-based alerting |
| API versioning | URL path, header-based, query param | URL path (`/api/v1/`) | Most explicit, easiest to route, industry standard |

---

## Changelog

- 2026-02-11: Initial FSD created from PRD-001 v1.0 -- Generated by Claude

---

*End of FSD-001: Platform Infrastructure & DevOps*
